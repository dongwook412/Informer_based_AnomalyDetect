{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Informer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaIHYx4_ECL"
   },
   "source": [
    "## Download code and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW9TS6jp_YXc"
   },
   "outputs": [],
   "source": [
    "# !pip install -r ./Informer2020/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./dataset/ETDataset/ETTm1.csv\")\n",
    "data.date = pd.to_datetime(data.date)\n",
    "data = data.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-07-01 00:00:00</th>\n",
       "      <td>5.827</td>\n",
       "      <td>2.009</td>\n",
       "      <td>1.599</td>\n",
       "      <td>0.462</td>\n",
       "      <td>4.203</td>\n",
       "      <td>1.340</td>\n",
       "      <td>30.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 00:15:00</th>\n",
       "      <td>5.760</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.264</td>\n",
       "      <td>1.401</td>\n",
       "      <td>30.459999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 00:30:00</th>\n",
       "      <td>5.760</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.391</td>\n",
       "      <td>4.234</td>\n",
       "      <td>1.310</td>\n",
       "      <td>30.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 00:45:00</th>\n",
       "      <td>5.760</td>\n",
       "      <td>1.942</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.234</td>\n",
       "      <td>1.310</td>\n",
       "      <td>27.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01 01:00:00</th>\n",
       "      <td>5.693</td>\n",
       "      <td>2.076</td>\n",
       "      <td>1.492</td>\n",
       "      <td>0.426</td>\n",
       "      <td>4.142</td>\n",
       "      <td>1.371</td>\n",
       "      <td>27.787001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26 18:45:00</th>\n",
       "      <td>9.310</td>\n",
       "      <td>3.550</td>\n",
       "      <td>5.437</td>\n",
       "      <td>1.670</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26 19:00:00</th>\n",
       "      <td>10.114</td>\n",
       "      <td>3.550</td>\n",
       "      <td>6.183</td>\n",
       "      <td>1.564</td>\n",
       "      <td>3.716</td>\n",
       "      <td>1.462</td>\n",
       "      <td>9.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26 19:15:00</th>\n",
       "      <td>10.784</td>\n",
       "      <td>3.349</td>\n",
       "      <td>7.000</td>\n",
       "      <td>1.635</td>\n",
       "      <td>3.746</td>\n",
       "      <td>1.432</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26 19:30:00</th>\n",
       "      <td>11.655</td>\n",
       "      <td>3.617</td>\n",
       "      <td>7.533</td>\n",
       "      <td>1.706</td>\n",
       "      <td>4.173</td>\n",
       "      <td>1.523</td>\n",
       "      <td>9.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26 19:45:00</th>\n",
       "      <td>12.994</td>\n",
       "      <td>3.818</td>\n",
       "      <td>8.244</td>\n",
       "      <td>1.777</td>\n",
       "      <td>4.721</td>\n",
       "      <td>1.523</td>\n",
       "      <td>9.778000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       HUFL   HULL   MUFL   MULL   LUFL   LULL         OT\n",
       "date                                                                     \n",
       "2016-07-01 00:00:00   5.827  2.009  1.599  0.462  4.203  1.340  30.531000\n",
       "2016-07-01 00:15:00   5.760  2.076  1.492  0.426  4.264  1.401  30.459999\n",
       "2016-07-01 00:30:00   5.760  1.942  1.492  0.391  4.234  1.310  30.038000\n",
       "2016-07-01 00:45:00   5.760  1.942  1.492  0.426  4.234  1.310  27.013000\n",
       "2016-07-01 01:00:00   5.693  2.076  1.492  0.426  4.142  1.371  27.787001\n",
       "...                     ...    ...    ...    ...    ...    ...        ...\n",
       "2018-06-26 18:45:00   9.310  3.550  5.437  1.670  3.868  1.462   9.567000\n",
       "2018-06-26 19:00:00  10.114  3.550  6.183  1.564  3.716  1.462   9.567000\n",
       "2018-06-26 19:15:00  10.784  3.349  7.000  1.635  3.746  1.432   9.426000\n",
       "2018-06-26 19:30:00  11.655  3.617  7.533  1.706  4.173  1.523   9.426000\n",
       "2018-06-26 19:45:00  12.994  3.818  8.244  1.777  4.721  1.523   9.778000\n",
       "\n",
       "[69680 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data.iloc[:10*30*24*4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df)\n",
    "trans_df = scaler.transform(data)\n",
    "norm_data = pd.DataFrame(data=trans_df, index=data.index, columns=data.columns, dtype=None, copy=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HUFL</th>\n",
       "      <th>HULL</th>\n",
       "      <th>MUFL</th>\n",
       "      <th>MULL</th>\n",
       "      <th>LUFL</th>\n",
       "      <th>LULL</th>\n",
       "      <th>OT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "      <td>69680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.294849</td>\n",
       "      <td>0.101654</td>\n",
       "      <td>-0.358719</td>\n",
       "      <td>0.107309</td>\n",
       "      <td>0.225105</td>\n",
       "      <td>0.042873</td>\n",
       "      <td>-0.372737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.383776</td>\n",
       "      <td>0.917197</td>\n",
       "      <td>1.449403</td>\n",
       "      <td>0.886133</td>\n",
       "      <td>1.069708</td>\n",
       "      <td>1.002302</td>\n",
       "      <td>0.861735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-6.284042</td>\n",
       "      <td>-3.472437</td>\n",
       "      <td>-6.872093</td>\n",
       "      <td>-3.294317</td>\n",
       "      <td>-3.666171</td>\n",
       "      <td>-3.677795</td>\n",
       "      <td>-2.137661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.604896</td>\n",
       "      <td>-0.553195</td>\n",
       "      <td>-0.574611</td>\n",
       "      <td>-0.471523</td>\n",
       "      <td>-0.474530</td>\n",
       "      <td>-0.270797</td>\n",
       "      <td>-1.012301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.016057</td>\n",
       "      <td>0.078551</td>\n",
       "      <td>-0.001573</td>\n",
       "      <td>0.137950</td>\n",
       "      <td>-0.002572</td>\n",
       "      <td>0.238333</td>\n",
       "      <td>-0.566382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.559692</td>\n",
       "      <td>0.740851</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>0.747913</td>\n",
       "      <td>0.746366</td>\n",
       "      <td>0.643968</td>\n",
       "      <td>0.106017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.980689</td>\n",
       "      <td>3.720301</td>\n",
       "      <td>2.562669</td>\n",
       "      <td>3.518243</td>\n",
       "      <td>5.158902</td>\n",
       "      <td>3.695409</td>\n",
       "      <td>2.915948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               HUFL          HULL          MUFL          MULL          LUFL  \\\n",
       "count  69680.000000  69680.000000  69680.000000  69680.000000  69680.000000   \n",
       "mean      -0.294849      0.101654     -0.358719      0.107309      0.225105   \n",
       "std        1.383776      0.917197      1.449403      0.886133      1.069708   \n",
       "min       -6.284042     -3.472437     -6.872093     -3.294317     -3.666171   \n",
       "25%       -0.604896     -0.553195     -0.574611     -0.471523     -0.474530   \n",
       "50%       -0.016057      0.078551     -0.001573      0.137950     -0.002572   \n",
       "75%        0.559692      0.740851      0.556609      0.747913      0.746366   \n",
       "max        2.980689      3.720301      2.562669      3.518243      5.158902   \n",
       "\n",
       "               LULL            OT  \n",
       "count  69680.000000  69680.000000  \n",
       "mean       0.042873     -0.372737  \n",
       "std        1.002302      0.861735  \n",
       "min       -3.677795     -2.137661  \n",
       "25%       -0.270797     -1.012301  \n",
       "50%        0.238333     -0.566382  \n",
       "75%        0.643968      0.106017  \n",
       "max        3.695409      2.915948  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/ETDataset/ETT_standard_scaler.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(scaler, './dataset/ETDataset/ETT_standard_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe"
   },
   "source": [
    "## Experiments: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RPdt-Kwc_RRZ"
   },
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6mx2dnwY9dWi"
   },
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'ETTm1' # data\n",
    "args.root_path = './dataset/ETDataset/' # root path of data file\n",
    "args.data_path = 'ETTm1.csv' # data file\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'OT' # target feature in S or MS task\n",
    "args.freq = 't' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "# 4*12=48, 4*12*2=96, 4*12*4=192, 4*12*8=384, 4*12*16=768\n",
    "args.seq_len = 4*12 # input sequence length of Informer encoder\n",
    "args.label_len = args.seq_len//2 # start token length of Informer decoder\n",
    "args.pred_len = 48 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 7 # encoder input size\n",
    "args.dec_in = 7 # decoder input size\n",
    "args.c_out = 7 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 8\n",
    "args.patience = 3\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본\n",
    "import time\n",
    "start = time.time()  # 시작 시간 저장start = time.time()  # 시작 시간 저장\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "f = open('./results/'+setting+ \"/time.txt\", 'w')\n",
    "f.write(str(time.time() - start))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서는 validation 함수 호출을 안했었음\n",
    "# learning rate이랑 s_layer, d_layer 바꿈\n",
    "# exp_information에서 train과 validatioin 함수에서 _get_data 함수의 flag를 'val'에서 'validation'으로 바꿈 --> 비슷해서 다시 원상복구('val'로 바꿔놓음)\n",
    "# 각 case 별 메모리가 언제 터지는지 확인\n",
    "\n",
    "# ProbSparse를 사용해도 빨라지지 않는 이유 -->  https://github.com/zhouhaoyi/Informer2020/issues/142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속도도 일단은 그냥 하는게 좋을듯. 어쨋든 distilling은 시간이 줄어드니까. probsparse는 속도안줄어든다고 적어놓고\n",
    "# 결과가 안좋았던 이유가 계속 label_len이 24로 고정임 --> 그래도 안좋네\n",
    "# lr type 2로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informer랑 informer stack으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "\n",
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'ETTm1' # data\n",
    "args.root_path = './dataset/ETDataset/' # root path of data file\n",
    "args.data_path = 'ETTm1.csv' # data file\n",
    "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'OT' # target feature in S or MS task\n",
    "args.freq = 't' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "# 4*12=48, 4*12*2=96, 4*12*4=192, 4*12*8=384, 4*12*16=768\n",
    "args.seq_len = 4*12 # input sequence length of Informer encoder\n",
    "args.label_len = args.seq_len//2 # start token length of Informer decoder\n",
    "args.pred_len = 48 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 7 # encoder input size\n",
    "args.dec_in = 7 # decoder input size\n",
    "args.c_out = 7 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "\n",
    "args.e_layers = 4 # num of encoder layers\n",
    "#args.s_layers = [3, 2, 1]\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "\n",
    "args.batch_size = 32 \n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type2'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 10\n",
    "args.patience = 5\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 34321\n",
      "val 11473\n",
      "test 11473\n",
      "\titers: 100, epoch: 1 | loss: 0.4589823\n",
      "\tspeed: 0.0810s/iter; left time: 860.7575s\n",
      "\titers: 200, epoch: 1 | loss: 0.2585064\n",
      "\tspeed: 0.0761s/iter; left time: 800.7519s\n",
      "\titers: 300, epoch: 1 | loss: 0.2602194\n",
      "\tspeed: 0.0763s/iter; left time: 794.8999s\n",
      "\titers: 400, epoch: 1 | loss: 0.2538326\n",
      "\tspeed: 0.0764s/iter; left time: 788.9585s\n",
      "\titers: 500, epoch: 1 | loss: 0.2383778\n",
      "\tspeed: 0.0776s/iter; left time: 793.5131s\n",
      "\titers: 600, epoch: 1 | loss: 0.1705451\n",
      "\tspeed: 0.0768s/iter; left time: 777.0149s\n",
      "\titers: 700, epoch: 1 | loss: 0.2315241\n",
      "\tspeed: 0.0769s/iter; left time: 770.1181s\n",
      "\titers: 800, epoch: 1 | loss: 0.1887291\n",
      "\tspeed: 0.0767s/iter; left time: 760.7995s\n",
      "\titers: 900, epoch: 1 | loss: 0.1729531\n",
      "\tspeed: 0.0810s/iter; left time: 795.7501s\n",
      "\titers: 1000, epoch: 1 | loss: 0.1727644\n",
      "\tspeed: 0.0802s/iter; left time: 779.9334s\n",
      "Epoch: 1 cost time: 83.52072429656982\n",
      "Epoch: 1, Steps: 1072 | Train Loss: 0.2396683 Vali Loss: 0.5743697 Test Loss: 0.4328639\n",
      "Validation loss decreased (inf --> 0.574370).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1395707\n",
      "\tspeed: 0.3591s/iter; left time: 3428.7675s\n",
      "\titers: 200, epoch: 2 | loss: 0.1272819\n",
      "\tspeed: 0.0774s/iter; left time: 730.9432s\n",
      "\titers: 300, epoch: 2 | loss: 0.1606536\n",
      "\tspeed: 0.0780s/iter; left time: 729.0414s\n",
      "\titers: 400, epoch: 2 | loss: 0.1164226\n",
      "\tspeed: 0.0780s/iter; left time: 721.0390s\n",
      "\titers: 500, epoch: 2 | loss: 0.0859534\n",
      "\tspeed: 0.0774s/iter; left time: 707.9351s\n",
      "\titers: 600, epoch: 2 | loss: 0.0971001\n",
      "\tspeed: 0.0776s/iter; left time: 701.8536s\n",
      "\titers: 700, epoch: 2 | loss: 0.1069338\n",
      "\tspeed: 0.0777s/iter; left time: 695.2838s\n",
      "\titers: 800, epoch: 2 | loss: 0.0954186\n",
      "\tspeed: 0.0787s/iter; left time: 696.7391s\n",
      "\titers: 900, epoch: 2 | loss: 0.1021505\n",
      "\tspeed: 0.0779s/iter; left time: 681.8198s\n",
      "\titers: 1000, epoch: 2 | loss: 0.0890814\n",
      "\tspeed: 0.0779s/iter; left time: 673.8549s\n",
      "Epoch: 2 cost time: 83.3934555053711\n",
      "Epoch: 2, Steps: 1072 | Train Loss: 0.1109479 Vali Loss: 0.6485027 Test Loss: 0.4424704\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0732437\n",
      "\tspeed: 0.3582s/iter; left time: 3036.1648s\n",
      "\titers: 200, epoch: 3 | loss: 0.0613649\n",
      "\tspeed: 0.0770s/iter; left time: 644.8304s\n",
      "\titers: 300, epoch: 3 | loss: 0.0642470\n",
      "\tspeed: 0.0772s/iter; left time: 639.1665s\n",
      "\titers: 400, epoch: 3 | loss: 0.0667096\n",
      "\tspeed: 0.0770s/iter; left time: 629.8617s\n",
      "\titers: 500, epoch: 3 | loss: 0.0664095\n",
      "\tspeed: 0.0823s/iter; left time: 665.0438s\n",
      "\titers: 600, epoch: 3 | loss: 0.0706862\n",
      "\tspeed: 0.0887s/iter; left time: 707.4370s\n",
      "\titers: 700, epoch: 3 | loss: 0.0636927\n",
      "\tspeed: 0.0855s/iter; left time: 673.7763s\n",
      "\titers: 800, epoch: 3 | loss: 0.0654604\n",
      "\tspeed: 0.0807s/iter; left time: 627.9671s\n",
      "\titers: 900, epoch: 3 | loss: 0.0576330\n",
      "\tspeed: 0.0818s/iter; left time: 627.9892s\n",
      "\titers: 1000, epoch: 3 | loss: 0.0668516\n",
      "\tspeed: 0.0808s/iter; left time: 612.1610s\n",
      "Epoch: 3 cost time: 86.93340992927551\n",
      "Epoch: 3, Steps: 1072 | Train Loss: 0.0673105 Vali Loss: 0.6099725 Test Loss: 0.4467498\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0512578\n",
      "\tspeed: 0.3755s/iter; left time: 2780.6816s\n",
      "\titers: 200, epoch: 4 | loss: 0.0503502\n",
      "\tspeed: 0.0791s/iter; left time: 577.5596s\n",
      "\titers: 300, epoch: 4 | loss: 0.0697140\n",
      "\tspeed: 0.0790s/iter; left time: 569.5234s\n",
      "\titers: 400, epoch: 4 | loss: 0.0686414\n",
      "\tspeed: 0.0791s/iter; left time: 562.0960s\n",
      "\titers: 500, epoch: 4 | loss: 0.0701113\n",
      "\tspeed: 0.0781s/iter; left time: 547.0950s\n",
      "\titers: 600, epoch: 4 | loss: 0.0519214\n",
      "\tspeed: 0.0792s/iter; left time: 546.8454s\n",
      "\titers: 700, epoch: 4 | loss: 0.0633271\n",
      "\tspeed: 0.0783s/iter; left time: 532.8568s\n",
      "\titers: 800, epoch: 4 | loss: 0.0533195\n",
      "\tspeed: 0.0816s/iter; left time: 547.2592s\n",
      "\titers: 900, epoch: 4 | loss: 0.0490022\n",
      "\tspeed: 0.0774s/iter; left time: 511.2625s\n",
      "\titers: 1000, epoch: 4 | loss: 0.0498588\n",
      "\tspeed: 0.0775s/iter; left time: 504.0377s\n",
      "Epoch: 4 cost time: 84.4724988937378\n",
      "Epoch: 4, Steps: 1072 | Train Loss: 0.0587671 Vali Loss: 0.6162156 Test Loss: 0.4742209\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0528278\n",
      "\tspeed: 0.3592s/iter; left time: 2274.5726s\n",
      "\titers: 200, epoch: 5 | loss: 0.0469744\n",
      "\tspeed: 0.0774s/iter; left time: 482.6285s\n",
      "\titers: 300, epoch: 5 | loss: 0.0477673\n",
      "\tspeed: 0.0775s/iter; left time: 475.2065s\n",
      "\titers: 400, epoch: 5 | loss: 0.0515504\n",
      "\tspeed: 0.0806s/iter; left time: 486.3783s\n",
      "\titers: 500, epoch: 5 | loss: 0.0434216\n",
      "\tspeed: 0.0785s/iter; left time: 465.9334s\n",
      "\titers: 600, epoch: 5 | loss: 0.0509824\n",
      "\tspeed: 0.0780s/iter; left time: 455.0787s\n",
      "\titers: 700, epoch: 5 | loss: 0.0522448\n",
      "\tspeed: 0.0777s/iter; left time: 445.7275s\n",
      "\titers: 800, epoch: 5 | loss: 0.0469105\n",
      "\tspeed: 0.0782s/iter; left time: 440.5542s\n",
      "\titers: 900, epoch: 5 | loss: 0.0501741\n",
      "\tspeed: 0.0781s/iter; left time: 432.1966s\n",
      "\titers: 1000, epoch: 5 | loss: 0.0467188\n",
      "\tspeed: 0.0780s/iter; left time: 423.5330s\n",
      "Epoch: 5 cost time: 83.78337073326111\n",
      "Epoch: 5, Steps: 1072 | Train Loss: 0.0480151 Vali Loss: 0.6163422 Test Loss: 0.4917870\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0553167\n",
      "\tspeed: 0.3648s/iter; left time: 1919.3328s\n",
      "\titers: 200, epoch: 6 | loss: 0.0421533\n",
      "\tspeed: 0.0768s/iter; left time: 396.2300s\n",
      "\titers: 300, epoch: 6 | loss: 0.0467010\n",
      "\tspeed: 0.0770s/iter; left time: 389.6672s\n",
      "\titers: 400, epoch: 6 | loss: 0.0425203\n",
      "\tspeed: 0.0772s/iter; left time: 383.2258s\n",
      "\titers: 500, epoch: 6 | loss: 0.0441461\n",
      "\tspeed: 0.0786s/iter; left time: 382.2962s\n",
      "\titers: 600, epoch: 6 | loss: 0.0429433\n",
      "\tspeed: 0.0795s/iter; left time: 378.5709s\n",
      "\titers: 700, epoch: 6 | loss: 0.0434914\n",
      "\tspeed: 0.0829s/iter; left time: 386.1672s\n",
      "\titers: 800, epoch: 6 | loss: 0.0569032\n",
      "\tspeed: 0.0775s/iter; left time: 353.6853s\n",
      "\titers: 900, epoch: 6 | loss: 0.0614522\n",
      "\tspeed: 0.0770s/iter; left time: 343.5292s\n",
      "\titers: 1000, epoch: 6 | loss: 0.0580229\n",
      "\tspeed: 0.0772s/iter; left time: 336.6050s\n",
      "Epoch: 6 cost time: 84.08740329742432\n",
      "Epoch: 6, Steps: 1072 | Train Loss: 0.0459790 Vali Loss: 0.6254338 Test Loss: 0.5032862\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informer_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.5742696523666382, mae:0.49641111493110657\n",
      ">>>>>>>testing : informer_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.43205884099006653, mae:0.42168670892715454\n",
      "test 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.4330490231513977, mae:0.42202991247177124\n"
     ]
    }
   ],
   "source": [
    "# 4*12*24 까지는 메모리 안터짐\n",
    "#seq_len_list = [4*12, 4*12*2, 4*12*4, 4*12*8, 4*12*16, 4*12*24]\n",
    "seq_len_list = [4*12*4]\n",
    "args.attn = 'prob'\n",
    "args.distil = True\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    args.seq_len = seq_len\n",
    "    args.label_len = args.seq_len//2\n",
    "    Exp = Exp_Informer\n",
    "\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                    args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "        # set experiments\n",
    "        exp = Exp(args)\n",
    "\n",
    "        # train\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        # validation\n",
    "        print('>>>>>>>validation : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.validation(setting)\n",
    "        # test\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    start = time.time()\n",
    "    exp.test(setting)\n",
    "    f = open('./results/'+setting+ \"/time.txt\", 'w')\n",
    "    f.write(str(time.time() - start))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3625029\n",
      "\tspeed: 0.0443s/iter; left time: 392.9335s\n",
      "\titers: 200, epoch: 1 | loss: 0.2542932\n",
      "\tspeed: 0.0401s/iter; left time: 352.0168s\n",
      "\titers: 300, epoch: 1 | loss: 0.2421058\n",
      "\tspeed: 0.0394s/iter; left time: 341.7303s\n",
      "\titers: 400, epoch: 1 | loss: 0.2103720\n",
      "\tspeed: 0.0392s/iter; left time: 335.6565s\n",
      "\titers: 500, epoch: 1 | loss: 0.1903096\n",
      "\tspeed: 0.0394s/iter; left time: 333.7224s\n",
      "\titers: 600, epoch: 1 | loss: 0.1997913\n",
      "\tspeed: 0.0394s/iter; left time: 329.4581s\n",
      "\titers: 700, epoch: 1 | loss: 0.2010699\n",
      "\tspeed: 0.0390s/iter; left time: 322.3992s\n",
      "\titers: 800, epoch: 1 | loss: 0.2286577\n",
      "\tspeed: 0.0393s/iter; left time: 320.7291s\n",
      "Epoch: 1 cost time: 35.79236388206482\n",
      "Epoch: 1, Steps: 897 | Train Loss: 0.2455541 Vali Loss: 0.8114886 Test Loss: 0.4691055\n",
      "Validation loss decreased (inf --> 0.811489).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1686725\n",
      "\tspeed: 0.1628s/iter; left time: 1298.5499s\n",
      "\titers: 200, epoch: 2 | loss: 0.1418105\n",
      "\tspeed: 0.0392s/iter; left time: 308.3199s\n",
      "\titers: 300, epoch: 2 | loss: 0.1574299\n",
      "\tspeed: 0.0390s/iter; left time: 303.2538s\n",
      "\titers: 400, epoch: 2 | loss: 0.1430888\n",
      "\tspeed: 0.0384s/iter; left time: 294.4039s\n",
      "\titers: 500, epoch: 2 | loss: 0.1482316\n",
      "\tspeed: 0.0391s/iter; left time: 296.0619s\n",
      "\titers: 600, epoch: 2 | loss: 0.1456182\n",
      "\tspeed: 0.0393s/iter; left time: 293.3847s\n",
      "\titers: 700, epoch: 2 | loss: 0.1594835\n",
      "\tspeed: 0.0392s/iter; left time: 288.9977s\n",
      "\titers: 800, epoch: 2 | loss: 0.1384185\n",
      "\tspeed: 0.0394s/iter; left time: 286.7394s\n",
      "Epoch: 2 cost time: 35.09425139427185\n",
      "Epoch: 2, Steps: 897 | Train Loss: 0.1561540 Vali Loss: 0.8905054 Test Loss: 0.4972381\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1230545\n",
      "\tspeed: 0.1631s/iter; left time: 1153.9960s\n",
      "\titers: 200, epoch: 3 | loss: 0.0916627\n",
      "\tspeed: 0.0390s/iter; left time: 272.0965s\n",
      "\titers: 300, epoch: 3 | loss: 0.0995752\n",
      "\tspeed: 0.0389s/iter; left time: 267.4494s\n",
      "\titers: 400, epoch: 3 | loss: 0.1016900\n",
      "\tspeed: 0.0392s/iter; left time: 265.8432s\n",
      "\titers: 500, epoch: 3 | loss: 0.1074137\n",
      "\tspeed: 0.0391s/iter; left time: 260.9486s\n",
      "\titers: 600, epoch: 3 | loss: 0.0834932\n",
      "\tspeed: 0.0394s/iter; left time: 258.8872s\n",
      "\titers: 700, epoch: 3 | loss: 0.1171791\n",
      "\tspeed: 0.0395s/iter; left time: 255.9988s\n",
      "\titers: 800, epoch: 3 | loss: 0.0999237\n",
      "\tspeed: 0.0398s/iter; left time: 253.6674s\n",
      "Epoch: 3 cost time: 35.268213510513306\n",
      "Epoch: 3, Steps: 897 | Train Loss: 0.1060886 Vali Loss: 0.7954732 Test Loss: 0.4871497\n",
      "Validation loss decreased (0.811489 --> 0.795473).  Saving model ...\n",
      "\titers: 100, epoch: 4 | loss: 0.1070663\n",
      "\tspeed: 0.1654s/iter; left time: 1022.2018s\n",
      "\titers: 200, epoch: 4 | loss: 0.0855611\n",
      "\tspeed: 0.0395s/iter; left time: 239.9775s\n",
      "\titers: 300, epoch: 4 | loss: 0.0792776\n",
      "\tspeed: 0.0394s/iter; left time: 235.8831s\n",
      "\titers: 400, epoch: 4 | loss: 0.0920152\n",
      "\tspeed: 0.0392s/iter; left time: 230.7738s\n",
      "\titers: 500, epoch: 4 | loss: 0.0907600\n",
      "\tspeed: 0.0388s/iter; left time: 224.4697s\n",
      "\titers: 600, epoch: 4 | loss: 0.0831442\n",
      "\tspeed: 0.0394s/iter; left time: 223.5311s\n",
      "\titers: 700, epoch: 4 | loss: 0.0742848\n",
      "\tspeed: 0.0393s/iter; left time: 219.3700s\n",
      "\titers: 800, epoch: 4 | loss: 0.0794596\n",
      "\tspeed: 0.0394s/iter; left time: 215.6923s\n",
      "Epoch: 4 cost time: 35.26452946662903\n",
      "Epoch: 4, Steps: 897 | Train Loss: 0.0871711 Vali Loss: 0.8121243 Test Loss: 0.4929149\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0669497\n",
      "\tspeed: 0.1638s/iter; left time: 865.5413s\n",
      "\titers: 200, epoch: 5 | loss: 0.0630296\n",
      "\tspeed: 0.0394s/iter; left time: 204.0615s\n",
      "\titers: 300, epoch: 5 | loss: 0.0678026\n",
      "\tspeed: 0.0389s/iter; left time: 197.6232s\n",
      "\titers: 400, epoch: 5 | loss: 0.0700700\n",
      "\tspeed: 0.0392s/iter; left time: 195.2269s\n",
      "\titers: 500, epoch: 5 | loss: 0.0584733\n",
      "\tspeed: 0.0391s/iter; left time: 191.1665s\n",
      "\titers: 600, epoch: 5 | loss: 0.0808515\n",
      "\tspeed: 0.0392s/iter; left time: 187.4716s\n",
      "\titers: 700, epoch: 5 | loss: 0.0564286\n",
      "\tspeed: 0.0391s/iter; left time: 183.0896s\n",
      "\titers: 800, epoch: 5 | loss: 0.0556442\n",
      "\tspeed: 0.0391s/iter; left time: 179.2349s\n",
      "Epoch: 5 cost time: 35.1560583114624\n",
      "Epoch: 5, Steps: 897 | Train Loss: 0.0676394 Vali Loss: 0.8393002 Test Loss: 0.5218398\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0645827\n",
      "\tspeed: 0.1634s/iter; left time: 716.7476s\n",
      "\titers: 200, epoch: 6 | loss: 0.0730002\n",
      "\tspeed: 0.0392s/iter; left time: 167.9641s\n",
      "\titers: 300, epoch: 6 | loss: 0.0642602\n",
      "\tspeed: 0.0394s/iter; left time: 164.7844s\n",
      "\titers: 400, epoch: 6 | loss: 0.0598817\n",
      "\tspeed: 0.0395s/iter; left time: 161.3347s\n",
      "\titers: 500, epoch: 6 | loss: 0.0525299\n",
      "\tspeed: 0.0396s/iter; left time: 157.8969s\n",
      "\titers: 600, epoch: 6 | loss: 0.0562476\n",
      "\tspeed: 0.0390s/iter; left time: 151.5034s\n",
      "\titers: 700, epoch: 6 | loss: 0.0663952\n",
      "\tspeed: 0.0391s/iter; left time: 148.0916s\n",
      "\titers: 800, epoch: 6 | loss: 0.0642608\n",
      "\tspeed: 0.0392s/iter; left time: 144.4454s\n",
      "Epoch: 6 cost time: 35.22551989555359\n",
      "Epoch: 6, Steps: 897 | Train Loss: 0.0636621 Vali Loss: 0.8306950 Test Loss: 0.5101358\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0596949\n",
      "\tspeed: 0.1633s/iter; left time: 569.7646s\n",
      "\titers: 200, epoch: 7 | loss: 0.0500980\n",
      "\tspeed: 0.0391s/iter; left time: 132.5031s\n",
      "\titers: 300, epoch: 7 | loss: 0.0679633\n",
      "\tspeed: 0.0393s/iter; left time: 129.3129s\n",
      "\titers: 400, epoch: 7 | loss: 0.0547424\n",
      "\tspeed: 0.0393s/iter; left time: 125.2829s\n",
      "\titers: 500, epoch: 7 | loss: 0.0675064\n",
      "\tspeed: 0.0390s/iter; left time: 120.4157s\n",
      "\titers: 600, epoch: 7 | loss: 0.0555392\n",
      "\tspeed: 0.0390s/iter; left time: 116.6959s\n",
      "\titers: 700, epoch: 7 | loss: 0.0515466\n",
      "\tspeed: 0.0396s/iter; left time: 114.2972s\n",
      "\titers: 800, epoch: 7 | loss: 0.0599090\n",
      "\tspeed: 0.0391s/iter; left time: 109.0979s\n",
      "Epoch: 7 cost time: 35.18471908569336\n",
      "Epoch: 7, Steps: 897 | Train Loss: 0.0602158 Vali Loss: 0.8525202 Test Loss: 0.5237848\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 8 | loss: 0.0689745\n",
      "\tspeed: 0.1625s/iter; left time: 421.1597s\n",
      "\titers: 200, epoch: 8 | loss: 0.0528919\n",
      "\tspeed: 0.0391s/iter; left time: 97.4477s\n",
      "\titers: 300, epoch: 8 | loss: 0.0538595\n",
      "\tspeed: 0.0389s/iter; left time: 93.1109s\n",
      "\titers: 400, epoch: 8 | loss: 0.0557275\n",
      "\tspeed: 0.0389s/iter; left time: 89.1529s\n",
      "\titers: 500, epoch: 8 | loss: 0.0551539\n",
      "\tspeed: 0.0386s/iter; left time: 84.6881s\n",
      "\titers: 600, epoch: 8 | loss: 0.0551925\n",
      "\tspeed: 0.0384s/iter; left time: 80.2486s\n",
      "\titers: 700, epoch: 8 | loss: 0.0791162\n",
      "\tspeed: 0.0387s/iter; left time: 77.0133s\n",
      "\titers: 800, epoch: 8 | loss: 0.0556381\n",
      "\tspeed: 0.0393s/iter; left time: 74.4319s\n",
      "Epoch: 8 cost time: 34.90142488479614\n",
      "Epoch: 8, Steps: 897 | Train Loss: 0.0585729 Vali Loss: 0.8445685 Test Loss: 0.5188904\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.7954133749008179, mae:0.5758005380630493\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.48714959621429443, mae:0.4296830892562866\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.48714959621429443, mae:0.4296830892562866\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28657\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.2484137\n",
      "\tspeed: 0.0519s/iter; left time: 459.0344s\n",
      "\titers: 200, epoch: 1 | loss: 0.3338106\n",
      "\tspeed: 0.0519s/iter; left time: 454.0493s\n",
      "\titers: 300, epoch: 1 | loss: 0.2247837\n",
      "\tspeed: 0.0520s/iter; left time: 449.6828s\n",
      "\titers: 400, epoch: 1 | loss: 0.1762511\n",
      "\tspeed: 0.0519s/iter; left time: 443.9150s\n",
      "\titers: 500, epoch: 1 | loss: 0.2008881\n",
      "\tspeed: 0.0519s/iter; left time: 439.0178s\n",
      "\titers: 600, epoch: 1 | loss: 0.1653576\n",
      "\tspeed: 0.0519s/iter; left time: 433.1164s\n",
      "\titers: 700, epoch: 1 | loss: 0.1949293\n",
      "\tspeed: 0.0519s/iter; left time: 428.4911s\n",
      "\titers: 800, epoch: 1 | loss: 0.1351453\n",
      "\tspeed: 0.0519s/iter; left time: 423.4350s\n",
      "Epoch: 1 cost time: 46.500046253204346\n",
      "Epoch: 1, Steps: 895 | Train Loss: 0.2229664 Vali Loss: 0.8809652 Test Loss: 0.4728471\n",
      "Validation loss decreased (inf --> 0.880965).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1279562\n",
      "\tspeed: 0.2112s/iter; left time: 1680.0216s\n",
      "\titers: 200, epoch: 2 | loss: 0.1327267\n",
      "\tspeed: 0.0523s/iter; left time: 410.9404s\n",
      "\titers: 300, epoch: 2 | loss: 0.1521790\n",
      "\tspeed: 0.0523s/iter; left time: 405.5759s\n",
      "\titers: 400, epoch: 2 | loss: 0.1316226\n",
      "\tspeed: 0.0523s/iter; left time: 400.3904s\n",
      "\titers: 500, epoch: 2 | loss: 0.0960006\n",
      "\tspeed: 0.0523s/iter; left time: 395.2313s\n",
      "\titers: 600, epoch: 2 | loss: 0.0929739\n",
      "\tspeed: 0.0524s/iter; left time: 390.6859s\n",
      "\titers: 700, epoch: 2 | loss: 0.1219398\n",
      "\tspeed: 0.0524s/iter; left time: 385.0899s\n",
      "\titers: 800, epoch: 2 | loss: 0.0848833\n",
      "\tspeed: 0.0523s/iter; left time: 379.7531s\n",
      "Epoch: 2 cost time: 46.83272695541382\n",
      "Epoch: 2, Steps: 895 | Train Loss: 0.1212486 Vali Loss: 0.7786477 Test Loss: 0.4933817\n",
      "Validation loss decreased (0.880965 --> 0.778648).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0865121\n",
      "\tspeed: 0.2122s/iter; left time: 1498.0249s\n",
      "\titers: 200, epoch: 3 | loss: 0.0797552\n",
      "\tspeed: 0.0522s/iter; left time: 363.4323s\n",
      "\titers: 300, epoch: 3 | loss: 0.0640403\n",
      "\tspeed: 0.0521s/iter; left time: 357.6007s\n",
      "\titers: 400, epoch: 3 | loss: 0.0745118\n",
      "\tspeed: 0.0522s/iter; left time: 352.9402s\n",
      "\titers: 500, epoch: 3 | loss: 0.0745466\n",
      "\tspeed: 0.0522s/iter; left time: 347.6605s\n",
      "\titers: 600, epoch: 3 | loss: 0.0694552\n",
      "\tspeed: 0.0522s/iter; left time: 342.4874s\n",
      "\titers: 700, epoch: 3 | loss: 0.0626322\n",
      "\tspeed: 0.0523s/iter; left time: 337.6453s\n",
      "\titers: 800, epoch: 3 | loss: 0.0721613\n",
      "\tspeed: 0.0523s/iter; left time: 332.4913s\n",
      "Epoch: 3 cost time: 46.73716711997986\n",
      "Epoch: 3, Steps: 895 | Train Loss: 0.0726407 Vali Loss: 0.7627059 Test Loss: 0.4657852\n",
      "Validation loss decreased (0.778648 --> 0.762706).  Saving model ...\n",
      "\titers: 100, epoch: 4 | loss: 0.0606094\n",
      "\tspeed: 0.2129s/iter; left time: 1312.8121s\n",
      "\titers: 200, epoch: 4 | loss: 0.0708774\n",
      "\tspeed: 0.0524s/iter; left time: 317.5560s\n",
      "\titers: 300, epoch: 4 | loss: 0.0533966\n",
      "\tspeed: 0.0523s/iter; left time: 312.2497s\n",
      "\titers: 400, epoch: 4 | loss: 0.0520017\n",
      "\tspeed: 0.0523s/iter; left time: 306.9805s\n",
      "\titers: 500, epoch: 4 | loss: 0.0657375\n",
      "\tspeed: 0.0523s/iter; left time: 301.7693s\n",
      "\titers: 600, epoch: 4 | loss: 0.0546036\n",
      "\tspeed: 0.0524s/iter; left time: 297.1549s\n",
      "\titers: 700, epoch: 4 | loss: 0.0618431\n",
      "\tspeed: 0.0524s/iter; left time: 291.7044s\n",
      "\titers: 800, epoch: 4 | loss: 0.0569165\n",
      "\tspeed: 0.0524s/iter; left time: 286.4718s\n",
      "Epoch: 4 cost time: 46.878079891204834\n",
      "Epoch: 4, Steps: 895 | Train Loss: 0.0603956 Vali Loss: 0.7529760 Test Loss: 0.4725521\n",
      "Validation loss decreased (0.762706 --> 0.752976).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0476069\n",
      "\tspeed: 0.2122s/iter; left time: 1118.4024s\n",
      "\titers: 200, epoch: 5 | loss: 0.0442253\n",
      "\tspeed: 0.0523s/iter; left time: 270.2151s\n",
      "\titers: 300, epoch: 5 | loss: 0.0420327\n",
      "\tspeed: 0.0523s/iter; left time: 265.4260s\n",
      "\titers: 400, epoch: 5 | loss: 0.0472754\n",
      "\tspeed: 0.0521s/iter; left time: 258.7712s\n",
      "\titers: 500, epoch: 5 | loss: 0.0521841\n",
      "\tspeed: 0.0519s/iter; left time: 252.9368s\n",
      "\titers: 600, epoch: 5 | loss: 0.0565630\n",
      "\tspeed: 0.0520s/iter; left time: 247.9345s\n",
      "\titers: 700, epoch: 5 | loss: 0.0461736\n",
      "\tspeed: 0.0520s/iter; left time: 242.8958s\n",
      "\titers: 800, epoch: 5 | loss: 0.0408982\n",
      "\tspeed: 0.0520s/iter; left time: 237.6111s\n",
      "Epoch: 5 cost time: 46.634541034698486\n",
      "Epoch: 5, Steps: 895 | Train Loss: 0.0484689 Vali Loss: 0.7588487 Test Loss: 0.4665413\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0471468\n",
      "\tspeed: 0.2098s/iter; left time: 917.9800s\n",
      "\titers: 200, epoch: 6 | loss: 0.0560500\n",
      "\tspeed: 0.0522s/iter; left time: 223.0555s\n",
      "\titers: 300, epoch: 6 | loss: 0.0407769\n",
      "\tspeed: 0.0521s/iter; left time: 217.4782s\n",
      "\titers: 400, epoch: 6 | loss: 0.0514852\n",
      "\tspeed: 0.0521s/iter; left time: 212.2833s\n",
      "\titers: 500, epoch: 6 | loss: 0.0479539\n",
      "\tspeed: 0.0522s/iter; left time: 207.4609s\n",
      "\titers: 600, epoch: 6 | loss: 0.0479391\n",
      "\tspeed: 0.0521s/iter; left time: 201.7667s\n",
      "\titers: 700, epoch: 6 | loss: 0.0483059\n",
      "\tspeed: 0.0522s/iter; left time: 196.9687s\n",
      "\titers: 800, epoch: 6 | loss: 0.0544399\n",
      "\tspeed: 0.0521s/iter; left time: 191.6521s\n",
      "Epoch: 6 cost time: 46.64093470573425\n",
      "Epoch: 6, Steps: 895 | Train Loss: 0.0459560 Vali Loss: 0.7593411 Test Loss: 0.4694599\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0416702\n",
      "\tspeed: 0.2105s/iter; left time: 732.8347s\n",
      "\titers: 200, epoch: 7 | loss: 0.0506279\n",
      "\tspeed: 0.0521s/iter; left time: 176.2086s\n",
      "\titers: 300, epoch: 7 | loss: 0.0448235\n",
      "\tspeed: 0.0520s/iter; left time: 170.6683s\n",
      "\titers: 400, epoch: 7 | loss: 0.0399660\n",
      "\tspeed: 0.0522s/iter; left time: 165.9360s\n",
      "\titers: 500, epoch: 7 | loss: 0.0425226\n",
      "\tspeed: 0.0522s/iter; left time: 160.7664s\n",
      "\titers: 600, epoch: 7 | loss: 0.0421276\n",
      "\tspeed: 0.0522s/iter; left time: 155.6389s\n",
      "\titers: 700, epoch: 7 | loss: 0.0443782\n",
      "\tspeed: 0.0522s/iter; left time: 150.4057s\n",
      "\titers: 800, epoch: 7 | loss: 0.0379106\n",
      "\tspeed: 0.0522s/iter; left time: 145.0318s\n",
      "Epoch: 7 cost time: 46.66948676109314\n",
      "Epoch: 7, Steps: 895 | Train Loss: 0.0438859 Vali Loss: 0.7606968 Test Loss: 0.4743650\n",
      "EarlyStopping counter: 3 out of 5\n",
      "\titers: 100, epoch: 8 | loss: 0.0433571\n",
      "\tspeed: 0.2104s/iter; left time: 544.1216s\n",
      "\titers: 200, epoch: 8 | loss: 0.0467099\n",
      "\tspeed: 0.0521s/iter; left time: 129.5281s\n",
      "\titers: 300, epoch: 8 | loss: 0.0484811\n",
      "\tspeed: 0.0522s/iter; left time: 124.4554s\n",
      "\titers: 400, epoch: 8 | loss: 0.0373917\n",
      "\tspeed: 0.0522s/iter; left time: 119.2853s\n",
      "\titers: 500, epoch: 8 | loss: 0.0457782\n",
      "\tspeed: 0.0521s/iter; left time: 113.9564s\n",
      "\titers: 600, epoch: 8 | loss: 0.0416252\n",
      "\tspeed: 0.0521s/iter; left time: 108.7106s\n",
      "\titers: 700, epoch: 8 | loss: 0.0440985\n",
      "\tspeed: 0.0521s/iter; left time: 103.5155s\n",
      "\titers: 800, epoch: 8 | loss: 0.0428657\n",
      "\tspeed: 0.0522s/iter; left time: 98.3698s\n",
      "Epoch: 8 cost time: 46.650394439697266\n",
      "Epoch: 8, Steps: 895 | Train Loss: 0.0428786 Vali Loss: 0.7662645 Test Loss: 0.4777092\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.0402293\n",
      "\tspeed: 0.2103s/iter; left time: 355.6511s\n",
      "\titers: 200, epoch: 9 | loss: 0.0409136\n",
      "\tspeed: 0.0521s/iter; left time: 82.9685s\n",
      "\titers: 300, epoch: 9 | loss: 0.0405760\n",
      "\tspeed: 0.0522s/iter; left time: 77.7860s\n",
      "\titers: 400, epoch: 9 | loss: 0.0320012\n",
      "\tspeed: 0.0521s/iter; left time: 72.4624s\n",
      "\titers: 500, epoch: 9 | loss: 0.0431220\n",
      "\tspeed: 0.0522s/iter; left time: 67.3572s\n",
      "\titers: 600, epoch: 9 | loss: 0.0399115\n",
      "\tspeed: 0.0521s/iter; left time: 62.0342s\n",
      "\titers: 700, epoch: 9 | loss: 0.0495545\n",
      "\tspeed: 0.0521s/iter; left time: 56.8092s\n",
      "\titers: 800, epoch: 9 | loss: 0.0364418\n",
      "\tspeed: 0.0521s/iter; left time: 51.6147s\n",
      "Epoch: 9 cost time: 46.65012192726135\n",
      "Epoch: 9, Steps: 895 | Train Loss: 0.0417922 Vali Loss: 0.7652524 Test Loss: 0.4763671\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.7526218295097351, mae:0.5612887144088745\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4725522994995117, mae:0.4392852187156677\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4725522994995117, mae:0.4392852187156677\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28561\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3476085\n",
      "\tspeed: 0.0852s/iter; left time: 751.5827s\n",
      "\titers: 200, epoch: 1 | loss: 0.2467084\n",
      "\tspeed: 0.0858s/iter; left time: 748.2573s\n",
      "\titers: 300, epoch: 1 | loss: 0.2160026\n",
      "\tspeed: 0.0859s/iter; left time: 740.2660s\n",
      "\titers: 400, epoch: 1 | loss: 0.2177430\n",
      "\tspeed: 0.0859s/iter; left time: 731.6524s\n",
      "\titers: 500, epoch: 1 | loss: 0.2340465\n",
      "\tspeed: 0.0859s/iter; left time: 723.1511s\n",
      "\titers: 600, epoch: 1 | loss: 0.1840160\n",
      "\tspeed: 0.0859s/iter; left time: 715.1559s\n",
      "\titers: 700, epoch: 1 | loss: 0.1592513\n",
      "\tspeed: 0.0858s/iter; left time: 705.7223s\n",
      "\titers: 800, epoch: 1 | loss: 0.1324496\n",
      "\tspeed: 0.0858s/iter; left time: 697.1740s\n",
      "Epoch: 1 cost time: 76.5868866443634\n",
      "Epoch: 1, Steps: 892 | Train Loss: 0.2195649 Vali Loss: 0.6478152 Test Loss: 0.4122759\n",
      "Validation loss decreased (inf --> 0.647815).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1156684\n",
      "\tspeed: 0.3279s/iter; left time: 2599.8524s\n",
      "\titers: 200, epoch: 2 | loss: 0.1150452\n",
      "\tspeed: 0.0859s/iter; left time: 672.6516s\n",
      "\titers: 300, epoch: 2 | loss: 0.0899159\n",
      "\tspeed: 0.0859s/iter; left time: 663.6530s\n",
      "\titers: 400, epoch: 2 | loss: 0.0835198\n",
      "\tspeed: 0.0859s/iter; left time: 655.0069s\n",
      "\titers: 500, epoch: 2 | loss: 0.1022100\n",
      "\tspeed: 0.0858s/iter; left time: 645.9671s\n",
      "\titers: 600, epoch: 2 | loss: 0.0774038\n",
      "\tspeed: 0.0859s/iter; left time: 638.0846s\n",
      "\titers: 700, epoch: 2 | loss: 0.0699691\n",
      "\tspeed: 0.0860s/iter; left time: 630.1307s\n",
      "\titers: 800, epoch: 2 | loss: 0.0754682\n",
      "\tspeed: 0.0858s/iter; left time: 620.3196s\n",
      "Epoch: 2 cost time: 76.61026620864868\n",
      "Epoch: 2, Steps: 892 | Train Loss: 0.1001670 Vali Loss: 0.6969498 Test Loss: 0.4389581\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0711555\n",
      "\tspeed: 0.3268s/iter; left time: 2299.7520s\n",
      "\titers: 200, epoch: 3 | loss: 0.0545866\n",
      "\tspeed: 0.0860s/iter; left time: 596.2641s\n",
      "\titers: 300, epoch: 3 | loss: 0.0583574\n",
      "\tspeed: 0.0858s/iter; left time: 586.3609s\n",
      "\titers: 400, epoch: 3 | loss: 0.0551112\n",
      "\tspeed: 0.0858s/iter; left time: 578.2377s\n",
      "\titers: 500, epoch: 3 | loss: 0.0516123\n",
      "\tspeed: 0.0858s/iter; left time: 569.7566s\n",
      "\titers: 600, epoch: 3 | loss: 0.0623746\n",
      "\tspeed: 0.0859s/iter; left time: 561.6957s\n",
      "\titers: 700, epoch: 3 | loss: 0.0608826\n",
      "\tspeed: 0.0859s/iter; left time: 553.1209s\n",
      "\titers: 800, epoch: 3 | loss: 0.0616775\n",
      "\tspeed: 0.0859s/iter; left time: 544.0470s\n",
      "Epoch: 3 cost time: 76.62024116516113\n",
      "Epoch: 3, Steps: 892 | Train Loss: 0.0608738 Vali Loss: 0.6984368 Test Loss: 0.4335116\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0602234\n",
      "\tspeed: 0.3268s/iter; left time: 2008.4165s\n",
      "\titers: 200, epoch: 4 | loss: 0.0526960\n",
      "\tspeed: 0.0859s/iter; left time: 519.3548s\n",
      "\titers: 300, epoch: 4 | loss: 0.0529097\n",
      "\tspeed: 0.0859s/iter; left time: 510.6078s\n",
      "\titers: 400, epoch: 4 | loss: 0.0504602\n",
      "\tspeed: 0.0858s/iter; left time: 501.7399s\n",
      "\titers: 500, epoch: 4 | loss: 0.0450075\n",
      "\tspeed: 0.0857s/iter; left time: 492.5916s\n",
      "\titers: 600, epoch: 4 | loss: 0.0516937\n",
      "\tspeed: 0.0859s/iter; left time: 484.9441s\n",
      "\titers: 700, epoch: 4 | loss: 0.0494442\n",
      "\tspeed: 0.0859s/iter; left time: 476.0665s\n",
      "\titers: 800, epoch: 4 | loss: 0.0474572\n",
      "\tspeed: 0.0858s/iter; left time: 467.1786s\n",
      "Epoch: 4 cost time: 76.58405947685242\n",
      "Epoch: 4, Steps: 892 | Train Loss: 0.0516709 Vali Loss: 0.7003703 Test Loss: 0.4518503\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0510135\n",
      "\tspeed: 0.3266s/iter; left time: 1715.5892s\n",
      "\titers: 200, epoch: 5 | loss: 0.0542412\n",
      "\tspeed: 0.0858s/iter; left time: 441.9916s\n",
      "\titers: 300, epoch: 5 | loss: 0.0412293\n",
      "\tspeed: 0.0858s/iter; left time: 433.6246s\n",
      "\titers: 400, epoch: 5 | loss: 0.0462450\n",
      "\tspeed: 0.0858s/iter; left time: 424.8549s\n",
      "\titers: 500, epoch: 5 | loss: 0.0420462\n",
      "\tspeed: 0.0859s/iter; left time: 416.9077s\n",
      "\titers: 600, epoch: 5 | loss: 0.0380403\n",
      "\tspeed: 0.0859s/iter; left time: 408.4112s\n",
      "\titers: 700, epoch: 5 | loss: 0.0471830\n",
      "\tspeed: 0.0859s/iter; left time: 399.6318s\n",
      "\titers: 800, epoch: 5 | loss: 0.0446245\n",
      "\tspeed: 0.0858s/iter; left time: 390.7006s\n",
      "Epoch: 5 cost time: 76.57298827171326\n",
      "Epoch: 5, Steps: 892 | Train Loss: 0.0425728 Vali Loss: 0.7081416 Test Loss: 0.4477220\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0443285\n",
      "\tspeed: 0.3264s/iter; left time: 1423.3904s\n",
      "\titers: 200, epoch: 6 | loss: 0.0428026\n",
      "\tspeed: 0.0858s/iter; left time: 365.5443s\n",
      "\titers: 300, epoch: 6 | loss: 0.0469985\n",
      "\tspeed: 0.0857s/iter; left time: 356.8004s\n",
      "\titers: 400, epoch: 6 | loss: 0.0404134\n",
      "\tspeed: 0.0858s/iter; left time: 348.5570s\n",
      "\titers: 500, epoch: 6 | loss: 0.0370844\n",
      "\tspeed: 0.0858s/iter; left time: 340.0325s\n",
      "\titers: 600, epoch: 6 | loss: 0.0380131\n",
      "\tspeed: 0.0858s/iter; left time: 331.2865s\n",
      "\titers: 700, epoch: 6 | loss: 0.0446580\n",
      "\tspeed: 0.0859s/iter; left time: 323.0235s\n",
      "\titers: 800, epoch: 6 | loss: 0.0385514\n",
      "\tspeed: 0.0859s/iter; left time: 314.5370s\n",
      "Epoch: 6 cost time: 76.56790065765381\n",
      "Epoch: 6, Steps: 892 | Train Loss: 0.0407342 Vali Loss: 0.7122405 Test Loss: 0.4490447\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.6476181745529175, mae:0.5350329279899597\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4122759699821472, mae:0.42409202456474304\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4122759699821472, mae:0.42409202456474304\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl384_ll192_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28369\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3112913\n",
      "\tspeed: 0.1256s/iter; left time: 1100.6732s\n",
      "\titers: 200, epoch: 1 | loss: 0.2395836\n",
      "\tspeed: 0.1267s/iter; left time: 1097.1689s\n",
      "\titers: 300, epoch: 1 | loss: 0.2321823\n",
      "\tspeed: 0.1267s/iter; left time: 1084.8795s\n",
      "\titers: 400, epoch: 1 | loss: 0.1718611\n",
      "\tspeed: 0.1268s/iter; left time: 1072.4803s\n",
      "\titers: 500, epoch: 1 | loss: 0.1922207\n",
      "\tspeed: 0.1268s/iter; left time: 1060.0218s\n",
      "\titers: 600, epoch: 1 | loss: 0.1839789\n",
      "\tspeed: 0.1267s/iter; left time: 1046.8237s\n",
      "\titers: 700, epoch: 1 | loss: 0.1553108\n",
      "\tspeed: 0.1268s/iter; left time: 1034.8976s\n",
      "\titers: 800, epoch: 1 | loss: 0.1269534\n",
      "\tspeed: 0.1268s/iter; left time: 1021.8279s\n",
      "Epoch: 1 cost time: 112.26890540122986\n",
      "Epoch: 1, Steps: 886 | Train Loss: 0.2237995 Vali Loss: 0.6202900 Test Loss: 0.4562319\n",
      "Validation loss decreased (inf --> 0.620290).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1257259\n",
      "\tspeed: 0.4627s/iter; left time: 3643.9925s\n",
      "\titers: 200, epoch: 2 | loss: 0.1105902\n",
      "\tspeed: 0.1270s/iter; left time: 987.0669s\n",
      "\titers: 300, epoch: 2 | loss: 0.1007626\n",
      "\tspeed: 0.1269s/iter; left time: 973.7316s\n",
      "\titers: 400, epoch: 2 | loss: 0.0818697\n",
      "\tspeed: 0.1269s/iter; left time: 961.0850s\n",
      "\titers: 500, epoch: 2 | loss: 0.0947545\n",
      "\tspeed: 0.1268s/iter; left time: 947.7656s\n",
      "\titers: 600, epoch: 2 | loss: 0.0957144\n",
      "\tspeed: 0.1268s/iter; left time: 935.4646s\n",
      "\titers: 700, epoch: 2 | loss: 0.0757342\n",
      "\tspeed: 0.1268s/iter; left time: 922.6882s\n",
      "\titers: 800, epoch: 2 | loss: 0.0752374\n",
      "\tspeed: 0.1268s/iter; left time: 909.8963s\n",
      "Epoch: 2 cost time: 112.41346096992493\n",
      "Epoch: 2, Steps: 886 | Train Loss: 0.0942181 Vali Loss: 0.6550294 Test Loss: 0.4347073\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0644695\n",
      "\tspeed: 0.4617s/iter; left time: 3226.8366s\n",
      "\titers: 200, epoch: 3 | loss: 0.0547122\n",
      "\tspeed: 0.1268s/iter; left time: 873.8023s\n",
      "\titers: 300, epoch: 3 | loss: 0.0672567\n",
      "\tspeed: 0.1268s/iter; left time: 860.9711s\n",
      "\titers: 400, epoch: 3 | loss: 0.0601517\n",
      "\tspeed: 0.1268s/iter; left time: 848.2199s\n",
      "\titers: 500, epoch: 3 | loss: 0.0640763\n",
      "\tspeed: 0.1268s/iter; left time: 835.6850s\n",
      "\titers: 600, epoch: 3 | loss: 0.0672070\n",
      "\tspeed: 0.1268s/iter; left time: 822.9653s\n",
      "\titers: 700, epoch: 3 | loss: 0.0505957\n",
      "\tspeed: 0.1268s/iter; left time: 810.2029s\n",
      "\titers: 800, epoch: 3 | loss: 0.0507655\n",
      "\tspeed: 0.1269s/iter; left time: 797.9031s\n",
      "Epoch: 3 cost time: 112.38510060310364\n",
      "Epoch: 3, Steps: 886 | Train Loss: 0.0591256 Vali Loss: 0.6876500 Test Loss: 0.4520161\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0590871\n",
      "\tspeed: 0.4613s/iter; left time: 2815.6142s\n",
      "\titers: 200, epoch: 4 | loss: 0.0523690\n",
      "\tspeed: 0.1268s/iter; left time: 761.3516s\n",
      "\titers: 300, epoch: 4 | loss: 0.0524574\n",
      "\tspeed: 0.1268s/iter; left time: 748.7695s\n",
      "\titers: 400, epoch: 4 | loss: 0.0484237\n",
      "\tspeed: 0.1268s/iter; left time: 735.9111s\n",
      "\titers: 500, epoch: 4 | loss: 0.0469878\n",
      "\tspeed: 0.1268s/iter; left time: 723.3329s\n",
      "\titers: 600, epoch: 4 | loss: 0.0506045\n",
      "\tspeed: 0.1269s/iter; left time: 710.9994s\n",
      "\titers: 700, epoch: 4 | loss: 0.0477912\n",
      "\tspeed: 0.1269s/iter; left time: 698.3627s\n",
      "\titers: 800, epoch: 4 | loss: 0.0459379\n",
      "\tspeed: 0.1269s/iter; left time: 685.4882s\n",
      "Epoch: 4 cost time: 112.39420342445374\n",
      "Epoch: 4, Steps: 886 | Train Loss: 0.0510961 Vali Loss: 0.6718940 Test Loss: 0.4426164\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0420055\n",
      "\tspeed: 0.4614s/iter; left time: 2407.2422s\n",
      "\titers: 200, epoch: 5 | loss: 0.0407641\n",
      "\tspeed: 0.1269s/iter; left time: 649.4102s\n",
      "\titers: 300, epoch: 5 | loss: 0.0411921\n",
      "\tspeed: 0.1268s/iter; left time: 636.1942s\n",
      "\titers: 400, epoch: 5 | loss: 0.0381866\n",
      "\tspeed: 0.1268s/iter; left time: 623.4080s\n",
      "\titers: 500, epoch: 5 | loss: 0.0475935\n",
      "\tspeed: 0.1270s/iter; left time: 611.7222s\n",
      "\titers: 600, epoch: 5 | loss: 0.0390924\n",
      "\tspeed: 0.1268s/iter; left time: 598.3248s\n",
      "\titers: 700, epoch: 5 | loss: 0.0435907\n",
      "\tspeed: 0.1269s/iter; left time: 585.9087s\n",
      "\titers: 800, epoch: 5 | loss: 0.0416683\n",
      "\tspeed: 0.1268s/iter; left time: 572.9593s\n",
      "Epoch: 5 cost time: 112.39982509613037\n",
      "Epoch: 5, Steps: 886 | Train Loss: 0.0420060 Vali Loss: 0.6725767 Test Loss: 0.4594015\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0379307\n",
      "\tspeed: 0.4612s/iter; left time: 1997.6709s\n",
      "\titers: 200, epoch: 6 | loss: 0.0461136\n",
      "\tspeed: 0.1269s/iter; left time: 536.7980s\n",
      "\titers: 300, epoch: 6 | loss: 0.0456333\n",
      "\tspeed: 0.1270s/iter; left time: 524.4318s\n",
      "\titers: 400, epoch: 6 | loss: 0.0392347\n",
      "\tspeed: 0.1269s/iter; left time: 511.6553s\n",
      "\titers: 500, epoch: 6 | loss: 0.0410716\n",
      "\tspeed: 0.1267s/iter; left time: 498.1709s\n",
      "\titers: 600, epoch: 6 | loss: 0.0334213\n",
      "\tspeed: 0.1269s/iter; left time: 486.2182s\n",
      "\titers: 700, epoch: 6 | loss: 0.0346858\n",
      "\tspeed: 0.1269s/iter; left time: 473.3922s\n",
      "\titers: 800, epoch: 6 | loss: 0.0383260\n",
      "\tspeed: 0.1269s/iter; left time: 460.6429s\n",
      "Epoch: 6 cost time: 112.40777063369751\n",
      "Epoch: 6, Steps: 886 | Train Loss: 0.0401734 Vali Loss: 0.6759720 Test Loss: 0.4646832\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl384_ll192_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.6199630498886108, mae:0.5341163277626038\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl384_ll192_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.45623186230659485, mae:0.4506562352180481\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.45623186230659485, mae:0.4506562352180481\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl768_ll384_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 27985\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3187859\n",
      "\tspeed: 0.2486s/iter; left time: 2148.1559s\n",
      "\titers: 200, epoch: 1 | loss: 0.3266142\n",
      "\tspeed: 0.2501s/iter; left time: 2135.7477s\n",
      "\titers: 300, epoch: 1 | loss: 0.2470279\n",
      "\tspeed: 0.2502s/iter; left time: 2112.0941s\n",
      "\titers: 400, epoch: 1 | loss: 0.1949244\n",
      "\tspeed: 0.2502s/iter; left time: 2086.8154s\n",
      "\titers: 500, epoch: 1 | loss: 0.2129897\n",
      "\tspeed: 0.2502s/iter; left time: 2061.5329s\n",
      "\titers: 600, epoch: 1 | loss: 0.1678238\n",
      "\tspeed: 0.2513s/iter; left time: 2045.4531s\n",
      "\titers: 700, epoch: 1 | loss: 0.1913889\n",
      "\tspeed: 0.2512s/iter; left time: 2019.7388s\n",
      "\titers: 800, epoch: 1 | loss: 0.1341497\n",
      "\tspeed: 0.2502s/iter; left time: 1986.5172s\n",
      "Epoch: 1 cost time: 218.84985876083374\n",
      "Epoch: 1, Steps: 874 | Train Loss: 0.2394952 Vali Loss: 0.7978891 Test Loss: 0.5034574\n",
      "Validation loss decreased (inf --> 0.797889).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1116303\n",
      "\tspeed: 0.8624s/iter; left time: 6698.4958s\n",
      "\titers: 200, epoch: 2 | loss: 0.1092560\n",
      "\tspeed: 0.2502s/iter; left time: 1918.3367s\n",
      "\titers: 300, epoch: 2 | loss: 0.1045428\n",
      "\tspeed: 0.2502s/iter; left time: 1893.2171s\n",
      "\titers: 400, epoch: 2 | loss: 0.1057584\n",
      "\tspeed: 0.2501s/iter; left time: 1867.8210s\n",
      "\titers: 500, epoch: 2 | loss: 0.0951334\n",
      "\tspeed: 0.2503s/iter; left time: 1843.8587s\n",
      "\titers: 600, epoch: 2 | loss: 0.0967570\n",
      "\tspeed: 0.2501s/iter; left time: 1817.7840s\n",
      "\titers: 700, epoch: 2 | loss: 0.0880601\n",
      "\tspeed: 0.2502s/iter; left time: 1793.2765s\n",
      "\titers: 800, epoch: 2 | loss: 0.0605127\n",
      "\tspeed: 0.2502s/iter; left time: 1768.1098s\n",
      "Epoch: 2 cost time: 218.693039894104\n",
      "Epoch: 2, Steps: 874 | Train Loss: 0.0988126 Vali Loss: 0.7350933 Test Loss: 0.4886445\n",
      "Validation loss decreased (0.797889 --> 0.735093).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0631809\n",
      "\tspeed: 0.8633s/iter; left time: 5950.8381s\n",
      "\titers: 200, epoch: 3 | loss: 0.0695111\n",
      "\tspeed: 0.2503s/iter; left time: 1699.9789s\n",
      "\titers: 300, epoch: 3 | loss: 0.0597042\n",
      "\tspeed: 0.2502s/iter; left time: 1674.6497s\n",
      "\titers: 400, epoch: 3 | loss: 0.0713863\n",
      "\tspeed: 0.2504s/iter; left time: 1650.5777s\n",
      "\titers: 500, epoch: 3 | loss: 0.0549334\n",
      "\tspeed: 0.2502s/iter; left time: 1624.6847s\n",
      "\titers: 600, epoch: 3 | loss: 0.0652977\n",
      "\tspeed: 0.2504s/iter; left time: 1600.5265s\n",
      "\titers: 700, epoch: 3 | loss: 0.0478811\n",
      "\tspeed: 0.2502s/iter; left time: 1574.4087s\n",
      "\titers: 800, epoch: 3 | loss: 0.0632706\n",
      "\tspeed: 0.2503s/iter; left time: 1549.8965s\n",
      "Epoch: 3 cost time: 218.7275950908661\n",
      "Epoch: 3, Steps: 874 | Train Loss: 0.0608022 Vali Loss: 0.7675278 Test Loss: 0.5318302\n",
      "EarlyStopping counter: 1 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0505645\n",
      "\tspeed: 0.8611s/iter; left time: 5183.1679s\n",
      "\titers: 200, epoch: 4 | loss: 0.0477260\n",
      "\tspeed: 0.2501s/iter; left time: 1480.2899s\n",
      "\titers: 300, epoch: 4 | loss: 0.0427004\n",
      "\tspeed: 0.2502s/iter; left time: 1455.9242s\n",
      "\titers: 400, epoch: 4 | loss: 0.0591366\n",
      "\tspeed: 0.2502s/iter; left time: 1430.9629s\n",
      "\titers: 500, epoch: 4 | loss: 0.0597265\n",
      "\tspeed: 0.2502s/iter; left time: 1405.8497s\n",
      "\titers: 600, epoch: 4 | loss: 0.0438038\n",
      "\tspeed: 0.2503s/iter; left time: 1381.4242s\n",
      "\titers: 700, epoch: 4 | loss: 0.0463739\n",
      "\tspeed: 0.2502s/iter; left time: 1355.6089s\n",
      "\titers: 800, epoch: 4 | loss: 0.0452886\n",
      "\tspeed: 0.2503s/iter; left time: 1331.4807s\n",
      "Epoch: 4 cost time: 218.67816281318665\n",
      "Epoch: 4, Steps: 874 | Train Loss: 0.0515369 Vali Loss: 0.7636996 Test Loss: 0.5423474\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0410523\n",
      "\tspeed: 0.8611s/iter; left time: 4430.1103s\n",
      "\titers: 200, epoch: 5 | loss: 0.0482882\n",
      "\tspeed: 0.2503s/iter; left time: 1262.6716s\n",
      "\titers: 300, epoch: 5 | loss: 0.0426277\n",
      "\tspeed: 0.2502s/iter; left time: 1237.0651s\n",
      "\titers: 400, epoch: 5 | loss: 0.0408790\n",
      "\tspeed: 0.2502s/iter; left time: 1212.0851s\n",
      "\titers: 500, epoch: 5 | loss: 0.0404098\n",
      "\tspeed: 0.2502s/iter; left time: 1187.2012s\n",
      "\titers: 600, epoch: 5 | loss: 0.0442520\n",
      "\tspeed: 0.2501s/iter; left time: 1161.6944s\n",
      "\titers: 700, epoch: 5 | loss: 0.0413303\n",
      "\tspeed: 0.2502s/iter; left time: 1137.2154s\n",
      "\titers: 800, epoch: 5 | loss: 0.0380534\n",
      "\tspeed: 0.2502s/iter; left time: 1112.2378s\n",
      "Epoch: 5 cost time: 218.6725115776062\n",
      "Epoch: 5, Steps: 874 | Train Loss: 0.0421832 Vali Loss: 0.7569913 Test Loss: 0.5435339\n",
      "EarlyStopping counter: 3 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0419663\n",
      "\tspeed: 0.8613s/iter; left time: 3678.7661s\n",
      "\titers: 200, epoch: 6 | loss: 0.0416409\n",
      "\tspeed: 0.2502s/iter; left time: 1043.5492s\n",
      "\titers: 300, epoch: 6 | loss: 0.0442352\n",
      "\tspeed: 0.2502s/iter; left time: 1018.5189s\n",
      "\titers: 400, epoch: 6 | loss: 0.0462748\n",
      "\tspeed: 0.2502s/iter; left time: 993.6195s\n",
      "\titers: 500, epoch: 6 | loss: 0.0418433\n",
      "\tspeed: 0.2502s/iter; left time: 968.5624s\n",
      "\titers: 600, epoch: 6 | loss: 0.0486216\n",
      "\tspeed: 0.2503s/iter; left time: 943.8657s\n",
      "\titers: 700, epoch: 6 | loss: 0.0378880\n",
      "\tspeed: 0.2503s/iter; left time: 918.8321s\n",
      "\titers: 800, epoch: 6 | loss: 0.0388612\n",
      "\tspeed: 0.2502s/iter; left time: 893.5768s\n",
      "Epoch: 6 cost time: 218.72274374961853\n",
      "Epoch: 6, Steps: 874 | Train Loss: 0.0402925 Vali Loss: 0.7485592 Test Loss: 0.5517346\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 5e-06\n",
      "\titers: 100, epoch: 7 | loss: 0.0338937\n",
      "\tspeed: 0.8707s/iter; left time: 2957.6021s\n",
      "\titers: 200, epoch: 7 | loss: 0.0334824\n",
      "\tspeed: 0.2502s/iter; left time: 824.9066s\n",
      "\titers: 300, epoch: 7 | loss: 0.0357992\n",
      "\tspeed: 0.2502s/iter; left time: 799.9826s\n",
      "\titers: 400, epoch: 7 | loss: 0.0395172\n",
      "\tspeed: 0.2502s/iter; left time: 774.8422s\n",
      "\titers: 500, epoch: 7 | loss: 0.0412258\n",
      "\tspeed: 0.2504s/iter; left time: 750.3460s\n",
      "\titers: 600, epoch: 7 | loss: 0.0440000\n",
      "\tspeed: 0.2502s/iter; left time: 724.8177s\n",
      "\titers: 700, epoch: 7 | loss: 0.0346835\n",
      "\tspeed: 0.2502s/iter; left time: 699.9141s\n",
      "\titers: 800, epoch: 7 | loss: 0.0365742\n",
      "\tspeed: 0.2502s/iter; left time: 674.6856s\n",
      "Epoch: 7 cost time: 218.86606359481812\n",
      "Epoch: 7, Steps: 874 | Train Loss: 0.0385563 Vali Loss: 0.7532524 Test Loss: 0.5504016\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl768_ll384_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.734819769859314, mae:0.5847810506820679\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl768_ll384_pl48_dm512_nh8_elNone_dl2_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.48864439129829407, mae:0.46568652987480164\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.48864439129829407, mae:0.46568652987480164\n"
     ]
    }
   ],
   "source": [
    "# 4*12*16 까지는 메모리 안터짐\n",
    "seq_len_list = [4*12, 4*12*2, 4*12*4, 4*12*8, 4*12*16]\n",
    "args.attn = 'full'\n",
    "args.distil = True\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    args.seq_len = seq_len\n",
    "    args.label_len = args.seq_len//2\n",
    "    \n",
    "    Exp = Exp_Informer\n",
    "\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                    args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "        # set experiments\n",
    "        exp = Exp(args)\n",
    "\n",
    "        # train\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        # validation\n",
    "        print('>>>>>>>validation : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.validation(setting)\n",
    "        # test\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    start = time.time()\n",
    "    exp.test(setting)\n",
    "    f = open('./results/'+setting+ \"/time.txt\", 'w')\n",
    "    f.write(str(time.time() - start))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28705\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3182735\n",
      "\tspeed: 0.0536s/iter; left time: 475.5303s\n",
      "\titers: 200, epoch: 1 | loss: 0.2353503\n",
      "\tspeed: 0.0538s/iter; left time: 471.9450s\n",
      "\titers: 300, epoch: 1 | loss: 0.2513919\n",
      "\tspeed: 0.0539s/iter; left time: 467.1990s\n",
      "\titers: 400, epoch: 1 | loss: 0.2594191\n",
      "\tspeed: 0.0538s/iter; left time: 461.4334s\n",
      "\titers: 500, epoch: 1 | loss: 0.2291717\n",
      "\tspeed: 0.0540s/iter; left time: 457.6102s\n",
      "\titers: 600, epoch: 1 | loss: 0.2159271\n",
      "\tspeed: 0.0539s/iter; left time: 450.8291s\n",
      "\titers: 700, epoch: 1 | loss: 0.1787126\n",
      "\tspeed: 0.0541s/iter; left time: 447.3148s\n",
      "\titers: 800, epoch: 1 | loss: 0.1834159\n",
      "\tspeed: 0.0539s/iter; left time: 440.7814s\n",
      "Epoch: 1 cost time: 48.37083029747009\n",
      "Epoch: 1, Steps: 897 | Train Loss: 0.2566530 Vali Loss: 0.7703239 Test Loss: 0.4393870\n",
      "Validation loss decreased (inf --> 0.770324).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1683200\n",
      "\tspeed: 0.2311s/iter; left time: 1842.4110s\n",
      "\titers: 200, epoch: 2 | loss: 0.2211721\n",
      "\tspeed: 0.0539s/iter; left time: 424.2756s\n",
      "\titers: 300, epoch: 2 | loss: 0.1976983\n",
      "\tspeed: 0.0540s/iter; left time: 419.7973s\n",
      "\titers: 400, epoch: 2 | loss: 0.2301213\n",
      "\tspeed: 0.0539s/iter; left time: 413.5552s\n",
      "\titers: 500, epoch: 2 | loss: 0.1842918\n",
      "\tspeed: 0.0539s/iter; left time: 408.5097s\n",
      "\titers: 600, epoch: 2 | loss: 0.1751894\n",
      "\tspeed: 0.0532s/iter; left time: 397.9654s\n",
      "\titers: 700, epoch: 2 | loss: 0.1556706\n",
      "\tspeed: 0.0533s/iter; left time: 393.1448s\n",
      "\titers: 800, epoch: 2 | loss: 0.1755170\n",
      "\tspeed: 0.0534s/iter; left time: 388.5174s\n",
      "Epoch: 2 cost time: 48.192758560180664\n",
      "Epoch: 2, Steps: 897 | Train Loss: 0.1684970 Vali Loss: 0.8034101 Test Loss: 0.4713146\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.1155312\n",
      "\tspeed: 0.2294s/iter; left time: 1623.4479s\n",
      "\titers: 200, epoch: 3 | loss: 0.1202467\n",
      "\tspeed: 0.0537s/iter; left time: 374.5947s\n",
      "\titers: 300, epoch: 3 | loss: 0.1221949\n",
      "\tspeed: 0.0538s/iter; left time: 370.2408s\n",
      "\titers: 400, epoch: 3 | loss: 0.1236250\n",
      "\tspeed: 0.0537s/iter; left time: 364.0403s\n",
      "\titers: 500, epoch: 3 | loss: 0.0991968\n",
      "\tspeed: 0.0538s/iter; left time: 359.0545s\n",
      "\titers: 600, epoch: 3 | loss: 0.1120275\n",
      "\tspeed: 0.0536s/iter; left time: 352.4780s\n",
      "\titers: 700, epoch: 3 | loss: 0.1356293\n",
      "\tspeed: 0.0537s/iter; left time: 347.6471s\n",
      "\titers: 800, epoch: 3 | loss: 0.1159005\n",
      "\tspeed: 0.0536s/iter; left time: 341.4979s\n",
      "Epoch: 3 cost time: 48.14666771888733\n",
      "Epoch: 3, Steps: 897 | Train Loss: 0.1185871 Vali Loss: 0.8480392 Test Loss: 0.4939106\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.1031376\n",
      "\tspeed: 0.2287s/iter; left time: 1413.4869s\n",
      "\titers: 200, epoch: 4 | loss: 0.0846620\n",
      "\tspeed: 0.0535s/iter; left time: 325.0664s\n",
      "\titers: 300, epoch: 4 | loss: 0.1195388\n",
      "\tspeed: 0.0536s/iter; left time: 320.4311s\n",
      "\titers: 400, epoch: 4 | loss: 0.1144615\n",
      "\tspeed: 0.0535s/iter; left time: 314.6113s\n",
      "\titers: 500, epoch: 4 | loss: 0.1172303\n",
      "\tspeed: 0.0533s/iter; left time: 308.1339s\n",
      "\titers: 600, epoch: 4 | loss: 0.1026279\n",
      "\tspeed: 0.0532s/iter; left time: 302.2876s\n",
      "\titers: 700, epoch: 4 | loss: 0.1012521\n",
      "\tspeed: 0.0533s/iter; left time: 297.2832s\n",
      "\titers: 800, epoch: 4 | loss: 0.1018096\n",
      "\tspeed: 0.0533s/iter; left time: 292.1148s\n",
      "Epoch: 4 cost time: 47.91991305351257\n",
      "Epoch: 4, Steps: 897 | Train Loss: 0.1000517 Vali Loss: 0.8370784 Test Loss: 0.5207008\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0799241\n",
      "\tspeed: 0.2279s/iter; left time: 1204.2197s\n",
      "\titers: 200, epoch: 5 | loss: 0.0760614\n",
      "\tspeed: 0.0533s/iter; left time: 276.3345s\n",
      "\titers: 300, epoch: 5 | loss: 0.0946405\n",
      "\tspeed: 0.0534s/iter; left time: 271.2371s\n",
      "\titers: 400, epoch: 5 | loss: 0.0849154\n",
      "\tspeed: 0.0533s/iter; left time: 265.7449s\n",
      "\titers: 500, epoch: 5 | loss: 0.0774654\n",
      "\tspeed: 0.0533s/iter; left time: 260.2637s\n",
      "\titers: 600, epoch: 5 | loss: 0.0844973\n",
      "\tspeed: 0.0534s/iter; left time: 255.2311s\n",
      "\titers: 700, epoch: 5 | loss: 0.0701891\n",
      "\tspeed: 0.0538s/iter; left time: 252.0220s\n",
      "\titers: 800, epoch: 5 | loss: 0.0690057\n",
      "\tspeed: 0.0538s/iter; left time: 246.5024s\n",
      "Epoch: 5 cost time: 47.97870659828186\n",
      "Epoch: 5, Steps: 897 | Train Loss: 0.0784191 Vali Loss: 0.8330348 Test Loss: 0.5224801\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0659846\n",
      "\tspeed: 0.2293s/iter; left time: 1005.5558s\n",
      "\titers: 200, epoch: 6 | loss: 0.0773166\n",
      "\tspeed: 0.0536s/iter; left time: 229.7902s\n",
      "\titers: 300, epoch: 6 | loss: 0.0796346\n",
      "\tspeed: 0.0533s/iter; left time: 223.0252s\n",
      "\titers: 400, epoch: 6 | loss: 0.0762301\n",
      "\tspeed: 0.0532s/iter; left time: 217.3564s\n",
      "\titers: 500, epoch: 6 | loss: 0.0720170\n",
      "\tspeed: 0.0536s/iter; left time: 213.4505s\n",
      "\titers: 600, epoch: 6 | loss: 0.0753940\n",
      "\tspeed: 0.0537s/iter; left time: 208.5012s\n",
      "\titers: 700, epoch: 6 | loss: 0.0722983\n",
      "\tspeed: 0.0537s/iter; left time: 203.2232s\n",
      "\titers: 800, epoch: 6 | loss: 0.0759897\n",
      "\tspeed: 0.0537s/iter; left time: 197.8813s\n",
      "Epoch: 6 cost time: 48.04043459892273\n",
      "Epoch: 6, Steps: 897 | Train Loss: 0.0733559 Vali Loss: 0.8394857 Test Loss: 0.5285673\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.7701906561851501, mae:0.5592901110649109\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl48_ll24_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4394441246986389, mae:0.41121938824653625\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4396546185016632, mae:0.4112255275249481\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28657\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3159090\n",
      "\tspeed: 0.0710s/iter; left time: 628.2010s\n",
      "\titers: 200, epoch: 1 | loss: 0.3294320\n",
      "\tspeed: 0.0709s/iter; left time: 620.4724s\n",
      "\titers: 300, epoch: 1 | loss: 0.2704520\n",
      "\tspeed: 0.0711s/iter; left time: 615.0269s\n",
      "\titers: 400, epoch: 1 | loss: 0.1893092\n",
      "\tspeed: 0.0711s/iter; left time: 608.0536s\n",
      "\titers: 500, epoch: 1 | loss: 0.1772013\n",
      "\tspeed: 0.0711s/iter; left time: 600.8254s\n",
      "\titers: 600, epoch: 1 | loss: 0.1940540\n",
      "\tspeed: 0.0710s/iter; left time: 592.8769s\n",
      "\titers: 700, epoch: 1 | loss: 0.1979686\n",
      "\tspeed: 0.0712s/iter; left time: 587.1715s\n",
      "\titers: 800, epoch: 1 | loss: 0.1713665\n",
      "\tspeed: 0.0711s/iter; left time: 579.6062s\n",
      "Epoch: 1 cost time: 63.62797021865845\n",
      "Epoch: 1, Steps: 895 | Train Loss: 0.2372556 Vali Loss: 0.7050785 Test Loss: 0.3543859\n",
      "Validation loss decreased (inf --> 0.705078).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1327755\n",
      "\tspeed: 0.3069s/iter; left time: 2441.5394s\n",
      "\titers: 200, epoch: 2 | loss: 0.1610374\n",
      "\tspeed: 0.0708s/iter; left time: 555.8729s\n",
      "\titers: 300, epoch: 2 | loss: 0.1489111\n",
      "\tspeed: 0.0707s/iter; left time: 548.6712s\n",
      "\titers: 400, epoch: 2 | loss: 0.1066039\n",
      "\tspeed: 0.0707s/iter; left time: 541.1036s\n",
      "\titers: 500, epoch: 2 | loss: 0.1478933\n",
      "\tspeed: 0.0707s/iter; left time: 534.1927s\n",
      "\titers: 600, epoch: 2 | loss: 0.1186860\n",
      "\tspeed: 0.0706s/iter; left time: 526.3886s\n",
      "\titers: 700, epoch: 2 | loss: 0.1064527\n",
      "\tspeed: 0.0707s/iter; left time: 519.9726s\n",
      "\titers: 800, epoch: 2 | loss: 0.1176982\n",
      "\tspeed: 0.0707s/iter; left time: 512.9037s\n",
      "Epoch: 2 cost time: 63.34697985649109\n",
      "Epoch: 2, Steps: 895 | Train Loss: 0.1356654 Vali Loss: 0.8321630 Test Loss: 0.3965273\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0910285\n",
      "\tspeed: 0.3052s/iter; left time: 2154.9358s\n",
      "\titers: 200, epoch: 3 | loss: 0.0823261\n",
      "\tspeed: 0.0711s/iter; left time: 495.1668s\n",
      "\titers: 300, epoch: 3 | loss: 0.0771042\n",
      "\tspeed: 0.0711s/iter; left time: 488.0121s\n",
      "\titers: 400, epoch: 3 | loss: 0.0928298\n",
      "\tspeed: 0.0712s/iter; left time: 481.6466s\n",
      "\titers: 500, epoch: 3 | loss: 0.0840112\n",
      "\tspeed: 0.0711s/iter; left time: 473.8046s\n",
      "\titers: 600, epoch: 3 | loss: 0.0717862\n",
      "\tspeed: 0.0710s/iter; left time: 465.8337s\n",
      "\titers: 700, epoch: 3 | loss: 0.0800931\n",
      "\tspeed: 0.0725s/iter; left time: 468.1609s\n",
      "\titers: 800, epoch: 3 | loss: 0.0744817\n",
      "\tspeed: 0.0707s/iter; left time: 449.4733s\n",
      "Epoch: 3 cost time: 63.701332092285156\n",
      "Epoch: 3, Steps: 895 | Train Loss: 0.0865687 Vali Loss: 0.9002026 Test Loss: 0.4305006\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0738190\n",
      "\tspeed: 0.3041s/iter; left time: 1875.1160s\n",
      "\titers: 200, epoch: 4 | loss: 0.0698191\n",
      "\tspeed: 0.0706s/iter; left time: 428.3226s\n",
      "\titers: 300, epoch: 4 | loss: 0.0744888\n",
      "\tspeed: 0.0706s/iter; left time: 421.1583s\n",
      "\titers: 400, epoch: 4 | loss: 0.0645846\n",
      "\tspeed: 0.0707s/iter; left time: 414.4369s\n",
      "\titers: 500, epoch: 4 | loss: 0.1006388\n",
      "\tspeed: 0.0706s/iter; left time: 407.3368s\n",
      "\titers: 600, epoch: 4 | loss: 0.0718498\n",
      "\tspeed: 0.0706s/iter; left time: 399.9411s\n",
      "\titers: 700, epoch: 4 | loss: 0.0712714\n",
      "\tspeed: 0.0706s/iter; left time: 392.9546s\n",
      "\titers: 800, epoch: 4 | loss: 0.0602160\n",
      "\tspeed: 0.0707s/iter; left time: 386.3526s\n",
      "Epoch: 4 cost time: 63.21195578575134\n",
      "Epoch: 4, Steps: 895 | Train Loss: 0.0711206 Vali Loss: 0.8685009 Test Loss: 0.4461727\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0676896\n",
      "\tspeed: 0.3039s/iter; left time: 1601.9133s\n",
      "\titers: 200, epoch: 5 | loss: 0.0634766\n",
      "\tspeed: 0.0706s/iter; left time: 365.2513s\n",
      "\titers: 300, epoch: 5 | loss: 0.0505111\n",
      "\tspeed: 0.0724s/iter; left time: 366.9110s\n",
      "\titers: 400, epoch: 5 | loss: 0.0611317\n",
      "\tspeed: 0.0716s/iter; left time: 355.7875s\n",
      "\titers: 500, epoch: 5 | loss: 0.0544216\n",
      "\tspeed: 0.0705s/iter; left time: 343.5496s\n",
      "\titers: 600, epoch: 5 | loss: 0.0569137\n",
      "\tspeed: 0.0705s/iter; left time: 336.5493s\n",
      "\titers: 700, epoch: 5 | loss: 0.0520891\n",
      "\tspeed: 0.0705s/iter; left time: 329.3798s\n",
      "\titers: 800, epoch: 5 | loss: 0.0531926\n",
      "\tspeed: 0.0706s/iter; left time: 322.5751s\n",
      "Epoch: 5 cost time: 63.45189714431763\n",
      "Epoch: 5, Steps: 895 | Train Loss: 0.0577443 Vali Loss: 0.8776982 Test Loss: 0.4494512\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0499142\n",
      "\tspeed: 0.3051s/iter; left time: 1334.9205s\n",
      "\titers: 200, epoch: 6 | loss: 0.0535943\n",
      "\tspeed: 0.0710s/iter; left time: 303.6940s\n",
      "\titers: 300, epoch: 6 | loss: 0.0499603\n",
      "\tspeed: 0.0711s/iter; left time: 296.8761s\n",
      "\titers: 400, epoch: 6 | loss: 0.0512996\n",
      "\tspeed: 0.0712s/iter; left time: 290.1609s\n",
      "\titers: 500, epoch: 6 | loss: 0.0534524\n",
      "\tspeed: 0.0711s/iter; left time: 282.6527s\n",
      "\titers: 600, epoch: 6 | loss: 0.0651949\n",
      "\tspeed: 0.0710s/iter; left time: 275.3418s\n",
      "\titers: 700, epoch: 6 | loss: 0.0562597\n",
      "\tspeed: 0.0711s/iter; left time: 268.4563s\n",
      "\titers: 800, epoch: 6 | loss: 0.0594099\n",
      "\tspeed: 0.0711s/iter; left time: 261.2646s\n",
      "Epoch: 6 cost time: 63.59639382362366\n",
      "Epoch: 6, Steps: 895 | Train Loss: 0.0548854 Vali Loss: 0.8771234 Test Loss: 0.4531098\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.7043785452842712, mae:0.5325689911842346\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl96_ll48_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.353873074054718, mae:0.38637620210647583\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.35411980748176575, mae:0.38642293214797974\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28561\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.3118942\n",
      "\tspeed: 0.1092s/iter; left time: 963.1020s\n",
      "\titers: 200, epoch: 1 | loss: 0.3788573\n",
      "\tspeed: 0.1097s/iter; left time: 956.2828s\n",
      "\titers: 300, epoch: 1 | loss: 0.2280937\n",
      "\tspeed: 0.1098s/iter; left time: 946.5658s\n",
      "\titers: 400, epoch: 1 | loss: 0.2179479\n",
      "\tspeed: 0.1100s/iter; left time: 937.6534s\n",
      "\titers: 500, epoch: 1 | loss: 0.2217664\n",
      "\tspeed: 0.1101s/iter; left time: 926.9745s\n",
      "\titers: 600, epoch: 1 | loss: 0.1871239\n",
      "\tspeed: 0.1100s/iter; left time: 914.8953s\n",
      "\titers: 700, epoch: 1 | loss: 0.2460075\n",
      "\tspeed: 0.1097s/iter; left time: 901.7387s\n",
      "\titers: 800, epoch: 1 | loss: 0.2301547\n",
      "\tspeed: 0.1098s/iter; left time: 891.5334s\n",
      "Epoch: 1 cost time: 98.01253628730774\n",
      "Epoch: 1, Steps: 892 | Train Loss: 0.2388292 Vali Loss: 0.6861487 Test Loss: 0.3994167\n",
      "Validation loss decreased (inf --> 0.686149).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1427238\n",
      "\tspeed: 0.4575s/iter; left time: 3627.8682s\n",
      "\titers: 200, epoch: 2 | loss: 0.1223744\n",
      "\tspeed: 0.1103s/iter; left time: 863.8783s\n",
      "\titers: 300, epoch: 2 | loss: 0.1248864\n",
      "\tspeed: 0.1101s/iter; left time: 851.3225s\n",
      "\titers: 400, epoch: 2 | loss: 0.1301037\n",
      "\tspeed: 0.1097s/iter; left time: 837.0382s\n",
      "\titers: 500, epoch: 2 | loss: 0.1338720\n",
      "\tspeed: 0.1097s/iter; left time: 825.8561s\n",
      "\titers: 600, epoch: 2 | loss: 0.1287726\n",
      "\tspeed: 0.1100s/iter; left time: 817.3853s\n",
      "\titers: 700, epoch: 2 | loss: 0.0968355\n",
      "\tspeed: 0.1098s/iter; left time: 804.8961s\n",
      "\titers: 800, epoch: 2 | loss: 0.0958235\n",
      "\tspeed: 0.1097s/iter; left time: 793.0519s\n",
      "Epoch: 2 cost time: 98.08656740188599\n",
      "Epoch: 2, Steps: 892 | Train Loss: 0.1311512 Vali Loss: 0.7618347 Test Loss: 0.4282108\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0905007\n",
      "\tspeed: 0.4563s/iter; left time: 3211.0515s\n",
      "\titers: 200, epoch: 3 | loss: 0.0900257\n",
      "\tspeed: 0.1102s/iter; left time: 764.2228s\n",
      "\titers: 300, epoch: 3 | loss: 0.0911543\n",
      "\tspeed: 0.1101s/iter; left time: 752.4131s\n",
      "\titers: 400, epoch: 3 | loss: 0.0767557\n",
      "\tspeed: 0.1100s/iter; left time: 741.3307s\n",
      "\titers: 500, epoch: 3 | loss: 0.0752928\n",
      "\tspeed: 0.1100s/iter; left time: 730.3733s\n",
      "\titers: 600, epoch: 3 | loss: 0.0887917\n",
      "\tspeed: 0.1098s/iter; left time: 718.0497s\n",
      "\titers: 700, epoch: 3 | loss: 0.0715551\n",
      "\tspeed: 0.1099s/iter; left time: 707.5410s\n",
      "\titers: 800, epoch: 3 | loss: 0.0650386\n",
      "\tspeed: 0.1097s/iter; left time: 695.4399s\n",
      "Epoch: 3 cost time: 98.10024285316467\n",
      "Epoch: 3, Steps: 892 | Train Loss: 0.0836191 Vali Loss: 0.8140503 Test Loss: 0.4676309\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0736837\n",
      "\tspeed: 0.4550s/iter; left time: 2796.2359s\n",
      "\titers: 200, epoch: 4 | loss: 0.0814884\n",
      "\tspeed: 0.1098s/iter; left time: 663.5426s\n",
      "\titers: 300, epoch: 4 | loss: 0.0658715\n",
      "\tspeed: 0.1098s/iter; left time: 652.6824s\n",
      "\titers: 400, epoch: 4 | loss: 0.0812354\n",
      "\tspeed: 0.1098s/iter; left time: 641.6688s\n",
      "\titers: 500, epoch: 4 | loss: 0.0720227\n",
      "\tspeed: 0.1097s/iter; left time: 630.0810s\n",
      "\titers: 600, epoch: 4 | loss: 0.0680269\n",
      "\tspeed: 0.1098s/iter; left time: 619.5418s\n",
      "\titers: 700, epoch: 4 | loss: 0.0814880\n",
      "\tspeed: 0.1099s/iter; left time: 609.6590s\n",
      "\titers: 800, epoch: 4 | loss: 0.0617901\n",
      "\tspeed: 0.1101s/iter; left time: 599.6905s\n",
      "Epoch: 4 cost time: 97.97873973846436\n",
      "Epoch: 4, Steps: 892 | Train Loss: 0.0699273 Vali Loss: 0.8172193 Test Loss: 0.4565950\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0497080\n",
      "\tspeed: 0.4555s/iter; left time: 2392.6463s\n",
      "\titers: 200, epoch: 5 | loss: 0.0506826\n",
      "\tspeed: 0.1100s/iter; left time: 566.5988s\n",
      "\titers: 300, epoch: 5 | loss: 0.0626674\n",
      "\tspeed: 0.1101s/iter; left time: 556.4952s\n",
      "\titers: 400, epoch: 5 | loss: 0.0698459\n",
      "\tspeed: 0.1102s/iter; left time: 545.7117s\n",
      "\titers: 500, epoch: 5 | loss: 0.0687858\n",
      "\tspeed: 0.1102s/iter; left time: 534.7404s\n",
      "\titers: 600, epoch: 5 | loss: 0.0522364\n",
      "\tspeed: 0.1100s/iter; left time: 522.7493s\n",
      "\titers: 700, epoch: 5 | loss: 0.0525935\n",
      "\tspeed: 0.1098s/iter; left time: 510.8716s\n",
      "\titers: 800, epoch: 5 | loss: 0.0571591\n",
      "\tspeed: 0.1098s/iter; left time: 499.9444s\n",
      "Epoch: 5 cost time: 98.08286809921265\n",
      "Epoch: 5, Steps: 892 | Train Loss: 0.0572735 Vali Loss: 0.8341761 Test Loss: 0.4738568\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0541477\n",
      "\tspeed: 0.4556s/iter; left time: 1986.7337s\n",
      "\titers: 200, epoch: 6 | loss: 0.0579156\n",
      "\tspeed: 0.1099s/iter; left time: 468.2636s\n",
      "\titers: 300, epoch: 6 | loss: 0.0488559\n",
      "\tspeed: 0.1100s/iter; left time: 457.6577s\n",
      "\titers: 400, epoch: 6 | loss: 0.0548469\n",
      "\tspeed: 0.1096s/iter; left time: 445.2588s\n",
      "\titers: 500, epoch: 6 | loss: 0.0452298\n",
      "\tspeed: 0.1096s/iter; left time: 434.2561s\n",
      "\titers: 600, epoch: 6 | loss: 0.0562686\n",
      "\tspeed: 0.1097s/iter; left time: 423.4542s\n",
      "\titers: 700, epoch: 6 | loss: 0.0459407\n",
      "\tspeed: 0.1100s/iter; left time: 413.7398s\n",
      "\titers: 800, epoch: 6 | loss: 0.0532227\n",
      "\tspeed: 0.1099s/iter; left time: 402.3946s\n",
      "Epoch: 6 cost time: 97.97169494628906\n",
      "Epoch: 6, Steps: 892 | Train Loss: 0.0546491 Vali Loss: 0.8505790 Test Loss: 0.4879689\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>validation : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "val 11473\n",
      "test shape: (358, 32, 48, 7) (358, 32, 48, 7)\n",
      "test shape: (11456, 48, 7) (11456, 48, 7)\n",
      "mse:0.6863567233085632, mae:0.5223574042320251\n",
      ">>>>>>>testing : informerstack_ETTm1_ftM_sl192_ll96_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.4004617929458618, mae:0.42156732082366943\n",
      "test 5713\n",
      "test shape: (178, 32, 48, 7) (178, 32, 48, 7)\n",
      "test shape: (5696, 48, 7) (5696, 48, 7)\n",
      "mse:0.3998092710971832, mae:0.42129629850387573\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informerstack_ETTm1_ftM_sl384_ll192_pl48_dm512_nh8_elNone_dl2_df2048_atprob_fc5_ebtimeF_dtFalse_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 28369\n",
      "val 11473\n",
      "test 5713\n",
      "\titers: 100, epoch: 1 | loss: 0.5122746\n",
      "\tspeed: 0.1679s/iter; left time: 1470.8923s\n",
      "\titers: 200, epoch: 1 | loss: 0.2606918\n",
      "\tspeed: 0.1685s/iter; left time: 1459.3521s\n",
      "\titers: 300, epoch: 1 | loss: 0.2430493\n",
      "\tspeed: 0.1684s/iter; left time: 1441.6700s\n",
      "\titers: 400, epoch: 1 | loss: 0.2317848\n",
      "\tspeed: 0.1686s/iter; left time: 1426.4572s\n",
      "\titers: 500, epoch: 1 | loss: 0.2194124\n",
      "\tspeed: 0.1687s/iter; left time: 1410.4084s\n",
      "\titers: 600, epoch: 1 | loss: 0.1561969\n",
      "\tspeed: 0.1685s/iter; left time: 1392.3372s\n",
      "\titers: 700, epoch: 1 | loss: 0.1731045\n",
      "\tspeed: 0.1690s/iter; left time: 1378.9715s\n",
      "\titers: 800, epoch: 1 | loss: 0.1742758\n",
      "\tspeed: 0.1687s/iter; left time: 1359.8111s\n",
      "Epoch: 1 cost time: 149.41142416000366\n",
      "Epoch: 1, Steps: 886 | Train Loss: 0.2516310 Vali Loss: 0.6613370 Test Loss: 0.4154284\n",
      "Validation loss decreased (inf --> 0.661337).  Saving model ...\n",
      "\titers: 100, epoch: 2 | loss: 0.1403399\n",
      "\tspeed: 0.7029s/iter; left time: 5535.5317s\n",
      "\titers: 200, epoch: 2 | loss: 0.1437083\n",
      "\tspeed: 0.1690s/iter; left time: 1314.0189s\n",
      "\titers: 300, epoch: 2 | loss: 0.1650145\n",
      "\tspeed: 0.1689s/iter; left time: 1296.3871s\n",
      "\titers: 400, epoch: 2 | loss: 0.1276709\n",
      "\tspeed: 0.1688s/iter; left time: 1278.8707s\n",
      "\titers: 500, epoch: 2 | loss: 0.1204654\n",
      "\tspeed: 0.1687s/iter; left time: 1261.1781s\n",
      "\titers: 600, epoch: 2 | loss: 0.1161184\n",
      "\tspeed: 0.1689s/iter; left time: 1245.9913s\n",
      "\titers: 700, epoch: 2 | loss: 0.1171355\n",
      "\tspeed: 0.1689s/iter; left time: 1228.6203s\n",
      "\titers: 800, epoch: 2 | loss: 0.1057951\n",
      "\tspeed: 0.1689s/iter; left time: 1211.7621s\n",
      "Epoch: 2 cost time: 149.6953194141388\n",
      "Epoch: 2, Steps: 886 | Train Loss: 0.1338187 Vali Loss: 0.6669307 Test Loss: 0.4372393\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0985138\n",
      "\tspeed: 0.7011s/iter; left time: 4900.0956s\n",
      "\titers: 200, epoch: 3 | loss: 0.1063391\n",
      "\tspeed: 0.1685s/iter; left time: 1160.9032s\n",
      "\titers: 300, epoch: 3 | loss: 0.0753799\n",
      "\tspeed: 0.1688s/iter; left time: 1145.8934s\n",
      "\titers: 400, epoch: 3 | loss: 0.0742239\n",
      "\tspeed: 0.1689s/iter; left time: 1129.4561s\n",
      "\titers: 500, epoch: 3 | loss: 0.0996271\n",
      "\tspeed: 0.1686s/iter; left time: 1110.9233s\n",
      "\titers: 600, epoch: 3 | loss: 0.0929609\n",
      "\tspeed: 0.1688s/iter; left time: 1095.1160s\n",
      "\titers: 700, epoch: 3 | loss: 0.0926096\n",
      "\tspeed: 0.1687s/iter; left time: 1078.0627s\n",
      "\titers: 800, epoch: 3 | loss: 0.0746699\n",
      "\tspeed: 0.1685s/iter; left time: 1059.3910s\n",
      "Epoch: 3 cost time: 149.4439857006073\n",
      "Epoch: 3, Steps: 886 | Train Loss: 0.0850332 Vali Loss: 0.7129690 Test Loss: 0.4766357\n",
      "EarlyStopping counter: 2 out of 5\n",
      "\titers: 100, epoch: 4 | loss: 0.0835223\n",
      "\tspeed: 0.7010s/iter; left time: 4278.4719s\n",
      "\titers: 200, epoch: 4 | loss: 0.0741441\n",
      "\tspeed: 0.1688s/iter; left time: 1013.1277s\n",
      "\titers: 300, epoch: 4 | loss: 0.0681415\n",
      "\tspeed: 0.1686s/iter; left time: 995.1927s\n",
      "\titers: 400, epoch: 4 | loss: 0.0666193\n",
      "\tspeed: 0.1690s/iter; left time: 980.6463s\n",
      "\titers: 500, epoch: 4 | loss: 0.0621133\n",
      "\tspeed: 0.1684s/iter; left time: 960.6120s\n",
      "\titers: 600, epoch: 4 | loss: 0.0642802\n",
      "\tspeed: 0.1685s/iter; left time: 944.2578s\n",
      "\titers: 700, epoch: 4 | loss: 0.0740470\n",
      "\tspeed: 0.1714s/iter; left time: 943.0151s\n",
      "\titers: 800, epoch: 4 | loss: 0.0772104\n",
      "\tspeed: 0.1777s/iter; left time: 960.1370s\n",
      "Epoch: 4 cost time: 151.30738592147827\n",
      "Epoch: 4, Steps: 886 | Train Loss: 0.0718960 Vali Loss: 0.7069331 Test Loss: 0.4716831\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0541553\n",
      "\tspeed: 0.7275s/iter; left time: 3795.2038s\n",
      "\titers: 200, epoch: 5 | loss: 0.0570980\n",
      "\tspeed: 0.1727s/iter; left time: 883.5551s\n",
      "\titers: 300, epoch: 5 | loss: 0.0538696\n",
      "\tspeed: 0.1726s/iter; left time: 866.0263s\n",
      "\titers: 400, epoch: 5 | loss: 0.0553440\n",
      "\tspeed: 0.1745s/iter; left time: 858.0359s\n",
      "\titers: 500, epoch: 5 | loss: 0.0575453\n",
      "\tspeed: 0.1771s/iter; left time: 852.9496s\n",
      "\titers: 600, epoch: 5 | loss: 0.0643630\n",
      "\tspeed: 0.1764s/iter; left time: 831.8494s\n",
      "\titers: 700, epoch: 5 | loss: 0.0590644\n",
      "\tspeed: 0.1696s/iter; left time: 783.1410s\n",
      "\titers: 800, epoch: 5 | loss: 0.0538762\n",
      "\tspeed: 0.1695s/iter; left time: 765.7220s\n",
      "Epoch: 5 cost time: 153.4225013256073\n",
      "Epoch: 5, Steps: 886 | Train Loss: 0.0597047 Vali Loss: 0.7443132 Test Loss: 0.5038899\n",
      "EarlyStopping counter: 4 out of 5\n",
      "\titers: 100, epoch: 6 | loss: 0.0647915\n",
      "\tspeed: 0.7151s/iter; left time: 3097.1703s\n",
      "\titers: 200, epoch: 6 | loss: 0.0764984\n",
      "\tspeed: 0.1698s/iter; left time: 718.3002s\n",
      "\titers: 300, epoch: 6 | loss: 0.0561007\n",
      "\tspeed: 0.1720s/iter; left time: 710.4415s\n",
      "\titers: 400, epoch: 6 | loss: 0.0638460\n",
      "\tspeed: 0.1713s/iter; left time: 690.5934s\n",
      "\titers: 500, epoch: 6 | loss: 0.0531489\n",
      "\tspeed: 0.1738s/iter; left time: 683.3904s\n",
      "\titers: 600, epoch: 6 | loss: 0.0572568\n",
      "\tspeed: 0.1736s/iter; left time: 665.0886s\n",
      "\titers: 700, epoch: 6 | loss: 0.0600416\n",
      "\tspeed: 0.1695s/iter; left time: 632.3701s\n",
      "\titers: 800, epoch: 6 | loss: 0.0597381\n",
      "\tspeed: 0.1713s/iter; left time: 621.9698s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7aff0168c574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/바탕화면/Informer2020/exp/exp_informer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                     \u001b[0mmodel_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/informer_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4*12*16 까지는 메모리 안터짐\n",
    "seq_len_list = [4*12, 4*12*2, 4*12*4, 4*12*8, 4*12*16]\n",
    "args.attn = 'prob'\n",
    "args.distil = False\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    args.seq_len = seq_len\n",
    "    args.label_len = args.seq_len//2\n",
    "    \n",
    "    Exp = Exp_Informer\n",
    "\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                    args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "        # set experiments\n",
    "        exp = Exp(args)\n",
    "\n",
    "        # train\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        # validation\n",
    "        print('>>>>>>>validation : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.validation(setting)\n",
    "        # test\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    start = time.time()\n",
    "    exp.test(setting)\n",
    "    f = open('./results/'+setting+ \"/time.txt\", 'w')\n",
    "    f.write(str(time.time() - start))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4*12*8 까지는 메모리 안터짐\n",
    "seq_len_list = [4*12, 4*12*2, 4*12*4, 4*12*8]\n",
    "args.attn = 'full'\n",
    "args.distil = False\n",
    "\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    args.seq_len = seq_len\n",
    "    args.label_len = args.seq_len//2\n",
    "    \n",
    "    Exp = Exp_Informer\n",
    "\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                    args.seq_len, args.label_len, args.pred_len,\n",
    "                    args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "        # set experiments\n",
    "        exp = Exp(args)\n",
    "\n",
    "        # train\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        # validation\n",
    "        print('>>>>>>>validation : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.validation(setting)\n",
    "        # test\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    start = time.time()\n",
    "    exp.test(setting)\n",
    "    f = open('./results/'+setting+ \"/time.txt\", 'w')\n",
    "    f.write(str(time.time() - start))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Informer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
