1. e_layers, d_layers 늘리기
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el8_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0
Epoch: 5, Steps: 3226 | Train Loss: 0.8783173 Vali Loss: 0.8970447 Test Loss: 0.9833966
val: mse:0.8971298336982727, mae:0.6432279944419861
test: mse:0.9832836985588074, mae:0.680682361125946

2. d_ff 늘림
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df4096_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0
Epoch: 9, Steps: 3226 | Train Loss: 0.8846588 Vali Loss: 0.9079936 Test Loss: 0.9896958
mse:0.90799880027771, mae:0.6426315307617188
mse:0.9897124171257019, mae:0.6808292865753174

3. dropout 늘림
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_dropout
Epoch: 6, Steps: 3226 | Train Loss: 0.8852806 Vali Loss: 0.8921714 Test Loss: 0.9718525
mse:0.8921703100204468, mae:0.6344845294952393
mse:0.9718538522720337, mae:0.6765305995941162

4. factor 늘림
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc10_ebtimeF_dtTrue_mxTrue_exp_0
Epoch: 6, Steps: 3226 | Train Loss: 0.8813182 Vali Loss: 0.8889262 Test Loss: 0.9715061
mse:0.8889258503913879, mae:0.6313537359237671
mse:0.9715059399604797, mae:0.6727418303489685

5. itr 늘림
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_itr
Epoch: 6, Steps: 3226 | Train Loss: 0.8820097 Vali Loss: 0.8886571 Test Loss: 0.9713087
mse:0.8886578679084778, mae:0.6331585645675659
mse:0.9713101387023926, mae:0.676213800907135

informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_1_itr
Epoch: 11, Steps: 3226 | Train Loss: 0.7122629 Vali Loss: 0.8005016 Test Loss: 0.8316644
mse:0.8008792400360107, mae:0.5580440163612366
mse:0.83161860704422, mae:0.5348519086837769

informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_2_itr
Epoch: 7, Steps: 3226 | Train Loss: 0.6838766 Vali Loss: 0.7714677 Test Loss: 0.8059770
mse:0.7713775038719177, mae:0.5596747994422913
mse:0.8060016632080078, mae:0.5633530020713806

6. 메모리 에러 남
7. output_attention를 True로 바꿈
informer_WADI_ftM_sl360_ll180_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_output_attention
Epoch: 1, Steps: 3226 | Train Loss: 0.8417625 Vali Loss: 0.8642466 Test Loss: 0.9084070
mse:0.864112913608551, mae:0.6142529249191284
mse:0.9085263609886169, mae:0.650412380695343

8. Informerstack s_layers=[3,2,1], d_layers=4 (lr=0.0001)
informerstack_WADI_ftM_sl360_ll180_pl60_dm512_nh16_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack1
Epoch: 9, Steps: 3226 | Train Loss: 0.5306491 Vali Loss: 0.6742322 Test Loss: 0.7836201
mse:0.673828125, mae:0.4646281898021698
mse:0.7832729816436768, mae:0.5013110041618347

9. Informerstack s_layers=[4,3,2,1], d_layers=4 (lr=0.0005), itr=10, output_attention = True, n_heads=8
informerstack_WADI_ftM_sl360_ll180_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack2
Epoch: 8, Steps: 3226 | Train Loss: 0.7744327 Vali Loss: 0.7841502 Test Loss: 0.8055329
mse:0.7841012477874756, mae:0.5639897584915161
mse:0.8055695295333862, mae:0.5818406343460083

10. Informerstack s_layers=[4,3,2,1], d_layers=4 (lr=0.0001), itr=5, output_attention = True, n_heads=16, seq_len = 6*60, label_len = 6*40, pred_len = 6*10
informerstack_WADI_ftM_sl360_ll240_pl60_dm512_nh16_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack3
Epoch: 11, Steps: 3226 | Train Loss: 0.5331054 Vali Loss: 0.6517778 Test Loss: 0.7329862
mse:0.6519488096237183, mae:0.44969168305397034
mse:0.7330784201622009, mae:0.45447438955307007

informerstack_WADI_ftM_sl360_ll240_pl60_dm512_nh16_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_1_informerstack3


11. Informerstack s_layers=[4,3,2,1], d_layers=4 (lr=0.0001), itr=0, output_attention = True, n_heads=24, lr을 0.6**으로 바꿈
informerstack_WADI_ftM_sl360_ll240_pl60_dm512_nh24_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack4
Epoch: 12, Steps: 3226 | Train Loss: 0.6054597 Vali Loss: 0.6817663 Test Loss: 0.7145563
중간에 멈춤

12. 
informerstack_WADI_ftS_sl480_ll240_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_uni2
2_FQ_601_PV(40번째 feature)
WADI_standard_normalised_normal_uni2.csv



==============================
내컴퓨터
1. informerstack_WADI_ftM_sl360_ll240_pl60_dm128_nh8_el2_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_y1
lradj type2로 변경. 데이터는 small 데이터 사용.
==> 별로임

2. informerstack_WADI_ftM_sl360_ll240_pl60_dm128_nh8_el2_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_y1
MinMaxScaler 사용해봤음. 데이터는 small 데이터 사용
==> epoch 2개하고 patience 1이라 종료됨, loss가 처음부터 굉장히 작은데 learning rate때문인가..

3. informerstack_WADI_ftM_sl360_ll240_pl60_dm128_nh8_el2_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0
MinMaxScaler 사용해봤음. 데이터는 small 데이터 사용. validation 개수 변경(2일). patience 5로 변경.
==> 별로임

4. informer_WADI_ftM_sl360_ll240_pl60_dm512_nh8_el4_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0
모델 내에 있는 Scaler 사용. 데이터는 feature 제거한 후 10초마다 바뀐 뒤 scaler안한 데이터(wadi_small) 사용
& informerstack 말고 informer 사용(e_layers = 4, d_layers = 2)
==> 얘도 0 근방으로 예측하긴 하지만 역동적이게 나옴

5.
모델 내에 있는 Scaler 사용. 데이터는 모두 NA인 4개 feature 제거한 후 10초마다 바뀐 뒤 scaler안한 데이터(wadi_allNA) 사용
& informerstack 말고 informer 사용(e_layers = 8, d_layers = 2) & seq_len 3시간, label_len 1시간, pred_len 10분
& d_model 1024 사용
==> batch_size 16개로 해야 돌아감.. 존나느림
==> 내꺼는 이거 돌리는중 그냥 거의 다 병신같이 나옴ㅋㅋ


황보컴퓨터
1. informerstack_WADI_ftS_sl480_ll240_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_uni2
univariate으로 seq_len, label_len, pred_len 늘려서 해보기
==> 별로임

2. informerstack_WADI_ftM_sl480_ll240_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_50
50개 feature로 사용. seq_len=480, label_len=240
==> 별로임

3. informerstack_SWaT_ftM_sl480_ll240_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_50
SWAT 사용. small 데이터 만들어서(6개)
==> 대략적인 추세는 잘 맞춤. 전체로 했을 때보다는 좀 별로인듯함

4. informerstack_WADI_ftM_sl480_ll240_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_trainsmall
type1, 12개 feature로 사용. train을 조금만 사용(7일).
==> 7일로만 데이터 normal 새롭게 구성해야했는데 잘못했네
==> epoch는 꽤 오래 하는데 결과는 안좋음

5. informer_WADI_ftM_sl480_ll240_pl60_dm512_nh8_el8_dl2_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_trainsmall
type1, 12개 feature로 사용. train을 조금만 사용(7일). informerstack 대신 informer 사용(e_layers = 8, d_layers = 2)
==> 7일로만 데이터 normal 새롭게 구성해야했는데 잘못했네
==> epoch는 꽤 오래 하는데 결과는 안좋음

6. 
type2, 93개 feature로 사용. train을 조금만 사용(7일. WADI_standard_normalised_normal_train7day.csv). informerstack 대신 informer 사용(e_layers = 4, d_layers = 2)
& seq_len 3시간, label_len 1시간

====================================================================================================================================================================================
1. informerstack_WADI_ftM_sl360_ll180_pl60_dm512_nh8_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_preprocessing
데이터 전처리(이상한 값 제거)한 걸로 시도
args.s_layers = [4,3,2,1]
Epoch: 1, Steps: 1876 | Train Loss: 0.1839212 Vali Loss: 0.2901632 Test Loss: 0.3327807
데이터 적게 학습

2. informerstack_WADI_ftM_sl360_ll180_pl60_dm512_nh16_elNone_dl4_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0_informerstack_preprocessing2
데이터 많이 학습
